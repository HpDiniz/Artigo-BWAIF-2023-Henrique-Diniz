{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HpDiniz/Artigo-BWAIF-2023-Henrique-Diniz/blob/main/Artigo_BWAIF_2023_Henrique_Diniz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z52BuxHJdJy-"
      },
      "outputs": [],
      "source": [
        "# Define a janela de treino em meses: 1 ou 6\n",
        "janela_treino = 1\n",
        "\n",
        "# Define qual a categoria será treinada: Geral, Papel, Tijolo ou Hibrido\n",
        "tipo_interesse = \"Hibrido\"\n",
        "\n",
        "# Define se os dados serão coletados ou obtidos de um csv armazenado no github\n",
        "consume_data_from_csv = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define as credenciais para utilização do MlFlow via DagsHub\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"\"\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"\""
      ],
      "metadata": {
        "id": "j781eHt9vAfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiywEkwhdHss"
      },
      "source": [
        "# 0. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFZouOfDMgM9"
      },
      "outputs": [],
      "source": [
        "import warnings;\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I3vDXARwdDR"
      },
      "outputs": [],
      "source": [
        "!pip install pystan --quiet\n",
        "!pip install statsmodels --quiet\n",
        "!pip install xgboost==1.6.2 --quiet\n",
        "!pip install pmdarima --quiet\n",
        "!pip install mysqlclient --quiet\n",
        "!pip install psycopg2-binary==2.8.6 --quiet\n",
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet\n",
        "!pip install unidecode --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDko7zAeR0zL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "import json\n",
        "import mlflow\n",
        "import requests\n",
        "import itertools\n",
        "\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import date\n",
        "from getpass import getpass\n",
        "from bs4 import BeautifulSoup\n",
        "from unidecode import unidecode\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rsm__ClwdDS"
      },
      "source": [
        "# 1. Read in Data and Process Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyp-wBpT4wr-"
      },
      "outputs": [],
      "source": [
        "this_month = \"2023-01\"\n",
        "last_month = \"2022-12\"\n",
        "\n",
        "headers = {\n",
        "    'User-Agent':\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36'\n",
        "        ' (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\n",
        "}\n",
        "\n",
        "experiment_name = f'{last_month}-{tipo_interesse}-{janela_treino}M'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsgUw0m7Xw0q"
      },
      "outputs": [],
      "source": [
        "def converteData(datas, monthYearOnly):\n",
        "\n",
        "    new_array = []\n",
        "    meses = [\"Janeiro\",\"Fevereiro\",\"Março\",\"Abril\",\"Maio\",\"Junho\",\"Julho\",\"Agosto\",\"Setembro\",\"Outubro\",\"Novembro\",\"Dezembro\"]\n",
        "\n",
        "    for data in datas:\n",
        "\n",
        "        item = data.split(\"/\")\n",
        "        mes = str(meses.index(item[0])+1)\n",
        "        mes = (\"0\" + mes)[len(mes)-1:len(mes)+1]\n",
        "\n",
        "        new_date = item[1] + \"-\" + mes\n",
        "\n",
        "        if not monthYearOnly:\n",
        "            new_date = new_date + \"-01 00:00:00\"\n",
        "        \n",
        "        new_array.append(new_date)\n",
        "        \n",
        "    return new_array\n",
        "\n",
        "def obtem_datas_faltantes(df, date_colun):\n",
        "\n",
        "    datas_faltantes = []\n",
        "    start_date = df[date_colun].min()\n",
        "    end_date = df[date_colun].max()\n",
        "\n",
        "    while(start_date < end_date):\n",
        "        date = str(start_date)[0:10]\n",
        "        df_aux = df[df[date_colun] == date]\n",
        "\n",
        "        if(len(df_aux) < 1):\n",
        "            datas_faltantes.append(date)\n",
        "\n",
        "        start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    return datas_faltantes\n",
        "\n",
        "def obtem_dados_mercado(indice):\n",
        "\n",
        "    indice = indice.lower()\n",
        "\n",
        "    if indice == \"igpm\":\n",
        "        indice = \"igp-m\"\n",
        "\n",
        "    response = requests.get('https://www.dadosdemercado.com.br/economia/' + indice, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df_igpm = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    anos = list(df_igpm.iloc[:, 0].values)\n",
        "\n",
        "    timestamp = []\n",
        "    values = []\n",
        "\n",
        "    for i in range(len(anos)):\n",
        "        for m in range(12, 0, -1):\n",
        "            taxa = str(list(df_igpm.iloc[:, m].values)[i])\n",
        "            if taxa != '--':\n",
        "                mes = str(m) if m > 9 else \"0\" + str(m)\n",
        "                timestamp.append(str(anos[i]) + \"-\" + mes)\n",
        "                values.append(round(float(taxa.replace(\"%\",\"\").replace(\",\",\".\")), 2))\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_tax = pd.DataFrame({\n",
        "        'Timestamp': timestamp,\n",
        "        'Value': values\n",
        "    })\n",
        "\n",
        "    df_tax['Value'] = pd.to_numeric(df_tax['Value'], downcast=\"float\")\n",
        "\n",
        "    return df_tax.replace(0, 0.01) \n",
        "\n",
        "def get_all_funds():\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/ranking', headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    idx = df[df['Setor'].isna()].index\n",
        "    df_funds = df.drop(idx)\n",
        "\n",
        "    df_funds = df_funds.rename(columns=lambda x: re.sub(r'Código\\s*do\\s*fundo', 'Ticker', x))\n",
        "    df_funds = df_funds.rename(columns=lambda x: re.sub(r'Quantidade\\s*Ativos', 'QuantidadeAtivos', x))\n",
        "\n",
        "    col_categorical = ['Ticker','Setor']\n",
        "    df_funds[col_categorical] = df_funds[col_categorical].astype('category')\n",
        "\n",
        "    df_funds.sort_values('Ticker', inplace=True)\n",
        "\n",
        "    df_funds = df_funds.drop_duplicates(subset=['Ticker']).replace('Títulos e Valores Mobiliários','Títulos e Val. Mob.')\n",
        "\n",
        "    df_funds = df_funds[['Ticker','Setor','QuantidadeAtivos']].reset_index(drop=True)\n",
        "\n",
        "    return df_funds\n",
        "\n",
        "def get_close(fund, years):\n",
        "\n",
        "    df_close = pd.DataFrame()\n",
        "\n",
        "    start_date = \"01-01-\" + str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) \n",
        "    end_date = pd.to_datetime('today').replace(day=1,hour=0,minute=0,second=0,microsecond=0).strftime(\"%d-%m-%Y\")\n",
        "\n",
        "    response = requests.get('https://fii-api.infomoney.com.br/api/v1/fii/cotacao/historico/grafico?Ticker='+fund+'&DataInicio='+start_date+'&DataFim='+end_date, headers=headers)\n",
        "\n",
        "    if not str(response.content) == \"b''\":\n",
        "\n",
        "        json_response = json.loads(response.content)\n",
        "\n",
        "        if 'errors' in json_response:\n",
        "            print(str(json_response['errors']))\n",
        "        else:\n",
        "            df_close = pd.read_json(json.dumps(json_response['dataValor']))\n",
        "\n",
        "            df_close['Ticker'] = fund\n",
        "            df_close['Ticker'] = df_close['Ticker'].astype('category')\n",
        "\n",
        "            df_close.rename(columns={'valor': 'Close'}, inplace = True)\n",
        "\n",
        "            df_close['Datetime'] = pd.to_datetime(df_close['data'], format='%d-%m-%YT%H:%M:%S')\n",
        "\n",
        "            df_close.drop(columns={'data'}, inplace = True)\n",
        "        \n",
        "    return df_close.replace(0, 0.01) \n",
        "\n",
        "def get_dividends(fund, years):\n",
        "\n",
        "    min_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01\"\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/funds/' + fund, headers=headers)\n",
        "\n",
        "    soup = bs4.BeautifulSoup(response.content, \"html\")\n",
        "    div = soup.find(\"div\", {\"id\": \"dividends-chart-wrapper\"})\n",
        "\n",
        "    labels = re.findall('\"labels\":\\[.*?\\]', str(div))\n",
        "    dividends = re.findall('\"data\":\\[.*?\\]', str(div))\n",
        "\n",
        "    dividends = json.loads(\"{\" + dividends[0] + \"}\")['data']\n",
        "    labels = json.loads(\"{\" + labels[0] + \"}\")['labels']\n",
        "\n",
        "    dates = converteData(labels, True)\n",
        "\n",
        "    result = []\n",
        "    if len(dates) > 0 and len(dates) == len(dividends):\n",
        "        for i in range(len(dates)):\n",
        "            if dates[i] >= min_date:\n",
        "                result.append({\n",
        "                    \"Ticker\": fund,\n",
        "                    \"Datetime\": dates[i],\n",
        "                    \"Dividends\": round(dividends[i],2)\n",
        "                })\n",
        "\n",
        "    df_dividends = pd.DataFrame(result)\n",
        "\n",
        "    return df_dividends.replace(0, 0.01) \n",
        "\n",
        "def get_adress(fundo):\n",
        "\n",
        "    api_url = \"https://fii-api.infomoney.com.br/api/v1/propertie/\" + fundo\n",
        "    response = requests.get(api_url)\n",
        "    data = []\n",
        "\n",
        "    if '{' in str(response.content):\n",
        "\n",
        "        response = response.json()\n",
        "\n",
        "        for item in response[\"property\"]:\n",
        "\n",
        "            row = {\n",
        "                \"Ticker\": fundo,\n",
        "                \"Tipo\": item[\"type\"],\n",
        "                \"Nome\": item[\"name\"],\n",
        "                \"DataCompra\": item[\"datePurchase\"],\n",
        "                \"ValorAreaBrutaLocavel\": item[\"valueGrossLeasableArea\"],\n",
        "                \"Estado\": item[\"state\"],\n",
        "                \"Cidade\": item[\"city\"],\n",
        "                \"Endereco\": item[\"address\"],\n",
        "                \"GoogleMapsLink\": item[\"googleMapsLink\"],\n",
        "                \"PercentualPartic\": item[\"percentagePartic\"],\n",
        "                \"PecentualVacancia\": item[\"percentVacancy\"],\n",
        "                \"PercentualInadimplencia90Dias\": item[\"percent90DayDeliquency\"],\n",
        "                \"PercentualFii\": item[\"percentFii\"],\n",
        "                \"Latitude\": float(\"NaN\"),\n",
        "                \"Longitude\": float(\"NaN\")\n",
        "            }\n",
        "\n",
        "            cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", item['googleMapsLink'])\n",
        "\n",
        "            if(len(cordinates) > 0):\n",
        "                cordinates = cordinates[0].split(\",\")\n",
        "                row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "            else:\n",
        "                \n",
        "                adress_url = (\"https://www.google.com/maps/place/\" + item[\"address\"] + \",\" + item[\"city\"] + \"-\" + item[\"state\"]).replace(\" \", \"%20\")\n",
        "\n",
        "                response = requests.get(adress_url)\n",
        "\n",
        "                cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", str(response.content))\n",
        "\n",
        "                if(len(cordinates) > 0):\n",
        "                    print(\"Endereço não encontrado, obtendo Latitude e Longitude aproximada...\")\n",
        "                    cordinates = cordinates[0].split(\",\")\n",
        "                    row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "                else:\n",
        "                    print(\"Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\")\n",
        "\n",
        "            data.append(row)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def filtra_tipo(df_history, df_funds, tipo):\n",
        "\n",
        "    tickers = list(df_funds['Ticker'].values)\n",
        "\n",
        "    if tipo == \"Papel\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] == \"Títulos e Val. Mob.\")]['Ticker'].values)\n",
        "    elif tipo == \"Tijolo\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] != \"Títulos e Val. Mob.\") & (df_funds[\"Setor\"] != \"Híbrido\")]['Ticker'].values)\n",
        "    elif tipo == \"Hibrido\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] == \"Híbrido\")]['Ticker'].values)\n",
        "    \n",
        "    return df_history[df_history['Ticker'].isin(tickers)]\n",
        "\n",
        "def get_month_close(df_close, date):\n",
        "\n",
        "    year = int(date.split('-')[0])\n",
        "    month = int(date.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month= month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "    end_date = (start_date + relativedelta(months=1))\n",
        "\n",
        "    df_aux = df_close.copy()\n",
        "\n",
        "    df_aux = df_aux[df_aux['Datetime'] >= start_date]\n",
        "    df_aux = df_aux[df_aux['Datetime'] < end_date]\n",
        "\n",
        "    if len(df_aux) > 0:\n",
        "        return float(df_aux.values[-1][0])\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def has_missing_data(df_history):\n",
        "\n",
        "    min = str(df_history['Datetime'].min())\n",
        "    max = str(df_history['Datetime'].max())\n",
        "\n",
        "    year = int(max.split('-')[0])\n",
        "    month = int(max.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month=month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "\n",
        "    while str(start_date.strftime(\"%Y-%m\")) != min:\n",
        "\n",
        "        if not str(start_date.strftime(\"%Y-%m\")) in list(df_history['Datetime']):\n",
        "            return True\n",
        "\n",
        "        start_date = (start_date - relativedelta(months=1))\n",
        "\n",
        "    return False\n",
        "\n",
        "def get_history(fund, years):\n",
        "\n",
        "    df_close = get_close(fund, years)\n",
        "    df_dividends = get_dividends(fund, years)\n",
        "\n",
        "    df_history = df_dividends.copy()\n",
        "\n",
        "    if len(df_history) > 0 and len(df_close) > 0:\n",
        "\n",
        "        new_df = []\n",
        "        for index, row in df_history.iterrows():\n",
        "\n",
        "            #print(\"Procurando 'Close' de: \" + row['Datetime'])\n",
        "            row['Dividends'] = round(row['Dividends'],2)\n",
        "            row['Close'] = get_month_close(df_close, row['Datetime'])\n",
        "            new_df.append(row)\n",
        "\n",
        "        df_history = pd.DataFrame(new_df)\n",
        "\n",
        "        datas = list(df_history['Datetime'])\n",
        "\n",
        "        if has_missing_data(df_history):\n",
        "            print(\"FII \" + fund + \" será removido por estar com dados faltantes.\")\n",
        "            df_history = pd.DataFrame()\n",
        "    \n",
        "    return df_history\n",
        "\n",
        "def process_daily_history(df_history, years):\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "\n",
        "    # Cria o histórico diário\n",
        "    df_history_daily = pd.DataFrame()\n",
        "\n",
        "    for fund in df_history['Ticker'].unique():\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_close = get_close(fund, years)\n",
        "\n",
        "        df_close[\"Datetime\"] = pd.to_datetime(df_close[\"Datetime\"], format=\"%Y-%m-%d\")\n",
        "\n",
        "        # Preenche os índices mensais\n",
        "        meses_percorridos = []\n",
        "\n",
        "        for index, row in df_close.iterrows():\n",
        "            \n",
        "            data_mes = str(row['Datetime'])[0:7]\n",
        "            df_aux = df_history[(df_history['Datetime'] == data_mes) & (df_history['Ticker'] == fund)]\n",
        "\n",
        "            if len(df_aux) < 1 or data_mes in meses_percorridos:\n",
        "                continue\n",
        "\n",
        "            meses_percorridos.append(data_mes)\n",
        "            df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), \"Dividends\"] = float(df_aux['Dividends'].values[0])\n",
        "            \n",
        "            for indice in indices:\n",
        "                df_aux = df_indices[indice][df_indices[indice]['Timestamp'] == data_mes]\n",
        "                df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), indice] = float(df_aux['Value'].values[0])\n",
        "            \n",
        "            df_history_daily = df_history_daily.append(df_close[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes))])\n",
        "\n",
        "    # Preenche o IFIX em todas as datas do histórico diário\n",
        "    datas_percorridos = []\n",
        "    for index, row in df_history_daily.iterrows():\n",
        "        \n",
        "        data = str(row[\"Datetime\"])[0:10]\n",
        "\n",
        "        print(\"Preenchendo IFIX em \" + data + \"...\")\n",
        "\n",
        "        if data not in datas_percorridos:\n",
        "\n",
        "            df_aux = df_ifix[df_ifix['Datetime'] == data]\n",
        "\n",
        "            if(len(df_aux) > 0):\n",
        "\n",
        "                df_history_daily.loc[(df_history_daily[\"Datetime\"].dt.strftime(\"%Y-%m-%d\").eq(data)), \"IFIX\"] = float(df_aux['Close'].values[0])\n",
        "                datas_percorridos.append(data)\n",
        "\n",
        "    return df_history_daily\n",
        "\n",
        "def preenche_historico_faltante(df_history_daily):\n",
        "\n",
        "    # Percorre todos os ativos do histórico\n",
        "    for ticker in df_history_daily['Ticker'].unique():\n",
        "\n",
        "        print(\"Adicionando dados faltantes de \" + ticker + \"...\")\n",
        "\n",
        "        # Obtém o histórico específico do ativo\n",
        "        df_aux = df_history_daily[df_history_daily['Ticker'] == ticker].copy()\n",
        "\n",
        "        # Obtém a menor e a maior data do histórico do ativo\n",
        "        start_date = pd.to_datetime(df_aux['Datetime']).min() + relativedelta(days=1)\n",
        "        end_date = pd.to_datetime(df_aux['Datetime']).max()\n",
        "\n",
        "        # Percorra todas as datas do intervalo\n",
        "        while(start_date < end_date):\n",
        "            \n",
        "            # Caso não haja algum registro no histórico para a data atual...\n",
        "            if (len(df_aux[df_aux['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(start_date)[0:10])]) < 1):\n",
        "                \n",
        "                # Obtém a data de ontém\n",
        "                ontem = (start_date - relativedelta(days=1))\n",
        "\n",
        "                # Obtém os registros de ontém\n",
        "                df_ontem = df_history_daily[(df_history_daily['Ticker'] == ticker) & (df_history_daily['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(ontem)[0:10]))]\n",
        "                \n",
        "                # Adiciona a data faltante no histórico\n",
        "                df_history_daily = df_history_daily.append(pd.DataFrame({\n",
        "                    \"Close\": df_ontem['Close'].values[0],\n",
        "                    \"Dividends\": df_ontem['Dividends'].values[0],\n",
        "                    \"Ticker\": [ticker],\n",
        "                    \"Datetime\": [start_date],\n",
        "                    \"Selic\": df_ontem['Selic'].values[0],\n",
        "                    \"IPCA\": df_ontem['IPCA'].values[0],\n",
        "                    \"IGPM\": df_ontem['IGPM'].values[0],\n",
        "                    \"IFIX\": df_ontem['IFIX'].values[0]\n",
        "                }))\n",
        "\n",
        "            # Incrementa a data de início\n",
        "            start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    # Ordena todos os registros pelo Ticker e Data\n",
        "    df_history_daily.sort_values(by=['Ticker', 'Datetime'], inplace = True)\n",
        "    df_history_daily = df_history_daily.reset_index(drop = True)\n",
        "    return df_history_daily\n",
        "\n",
        "def process_history(df_funds, years):\n",
        "\n",
        "    df_adress = pd.DataFrame()\n",
        "    df_history = pd.DataFrame()\n",
        "    \n",
        "    # Percorre a lista de fundos para obter o histórico individual de cada um deles\n",
        "    for fund in df_funds['Ticker']:\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_aux_1 = get_adress(fund)\n",
        "        df_aux_2 = get_history(fund, years)\n",
        "        \n",
        "        df_adress = df_adress.append(df_aux_1)\n",
        "        df_history = df_history.append(df_aux_2)\n",
        "\n",
        "        print(str(len(df_aux_2)) + \" dados de histórico e \" + str(len(df_aux_1)) + \" endereços foram encontrados.\")\n",
        "\n",
        "    is_NaN = df_history.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis=1)\n",
        "    rows_with_NaN = df_history[row_has_NaN]\n",
        "    tickers = rows_with_NaN['Ticker'].unique()\n",
        "    df_history = df_history[~df_history['Ticker'].isin(tickers)]\n",
        "\n",
        "    df_history = df_history[df_history['Datetime'] <= last_month]\n",
        "    df_history = df_history.drop_duplicates().replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    for fund in df_history[\"Ticker\"].unique():\n",
        "        if(len(df_history[df_history[\"Ticker\"] == fund]) < 12):\n",
        "            df_history = df_history[df_history[\"Ticker\"] != fund]\n",
        "\n",
        "    a = df_history[df_history['Datetime'] == last_month].Ticker.values\n",
        "    b = df_history.Ticker.unique()\n",
        "    intersection = list(set(a) & set(b))\n",
        "    fundos_faltantes = list(set(a) ^ set(b))\n",
        "\n",
        "    df_history = df_history[~df_history['Ticker'].isin(fundos_faltantes)]\n",
        "    \n",
        "    return df_history, df_adress\n",
        "\n",
        "def remove_big_variations(df, col_dict):\n",
        "\n",
        "    drop_indexes = []\n",
        "    for index, fundo in enumerate(df['Ticker'].unique()):\n",
        "        df_variacoes = df[df[\"Ticker\"] == fundo]\n",
        "\n",
        "        for key in col_dict:\n",
        "            df_variacoes = df_variacoes[(abs(df[key]) >= col_dict[key])]\n",
        "\n",
        "            if len(df_variacoes) > 0:\n",
        "                drop_indexes = drop_indexes + list(df[(df[\"Datetime\"] <= df_variacoes[\"Datetime\"].values[-1]) & (df.Ticker == fundo)].index)\n",
        "\n",
        "    df = df.drop(drop_indexes)\n",
        "    ticker_before = df[\"Ticker\"].unique()\n",
        "\n",
        "    # Remove fundos que não possuem pelo menos 20 registros\n",
        "    for fund in df[\"Ticker\"].unique():\n",
        "        if(len(df[df[\"Ticker\"] == fund]) < 20):\n",
        "            df = df[df[\"Ticker\"] != fund]\n",
        "\n",
        "    print(\"Tickers removidos: \", set(ticker_before) - set(df[\"Ticker\"].unique()))\n",
        "\n",
        "    return df\n",
        "\n",
        "def ajusta_desdobramento(df):\n",
        "    \n",
        "    # Desdobramentos obtidos em: https://br.investing.com/stock-split-calendar/\n",
        "    desdobramentos = {\n",
        "        \"BTCI11\": [\"2023-01\", 9],\n",
        "        \"CYCR11\": [\"2022-10\", 10],\n",
        "        \"EQIR11\": [\"2022-09\", 10],\n",
        "        \"VGIR11\": [\"2022-09\", 10],\n",
        "        \"GALG11\": [\"2022-08\", 10],\n",
        "        \"ARRI11\": [\"2022-08\", 10],\n",
        "        \"VIUR11\": [\"2022-05\", 10],\n",
        "        \"XPSF11\": [\"2022-05\", 10],\n",
        "        \"VIFI11\": [\"2022-04\", 10],\n",
        "        \"GAME11\": [\"2022-03\", 10],\n",
        "        \"BLMR11\": [\"2021-09\", 10],\n",
        "        \"MAXR11\": [\"2021-04\", 19],\n",
        "        \"RMAI11\": [\"2021-03\", 10],\n",
        "        \"FISC11\": [\"2020-12\", 10],\n",
        "        \"PQAG11\": [\"2020-11\", 10]\n",
        "    }\n",
        "\n",
        "    for key in desdobramentos:\n",
        "        if len(df[df[\"Ticker\"] == key]) > 0:\n",
        "            for index, row in df.iterrows():\n",
        "                if row[\"Ticker\"] == key and row[\"Datetime\"] < desdobramentos[key][0]:\n",
        "                    df.at[index,'Close'] = round(row['Close']/ desdobramentos[key][1],2)\n",
        "\n",
        "def getSectorMeans(df_funds, df_history):\n",
        "\n",
        "    df_setores = pd.DataFrame(({\n",
        "        'Setor':[],\n",
        "        'Datetime':[],\n",
        "        'DividendsChangeMean' :[],\n",
        "        'CloseChangeMean':[],\n",
        "        'DividendYieldChangeMean':[],\n",
        "        'DividendsChangeMean6M' :[],\n",
        "        'CloseChangeMean6M':[],\n",
        "        'DividendYieldChangeMean6M':[]\n",
        "    }))\n",
        "\n",
        "    for setor in df_funds[\"Setor\"].unique():\n",
        "\n",
        "        setor_tickers = df_funds[df_funds[\"Setor\"] == setor][\"Ticker\"].values\n",
        "\n",
        "        df_sector = df_history[df_history[\"Ticker\"].isin(setor_tickers)]\n",
        "        min_date = pd.to_datetime(df_sector[\"Datetime\"].min()).replace(day=1)\n",
        "        max_date = pd.to_datetime(df_sector[\"Datetime\"].max()).replace(day=1)\n",
        "\n",
        "        while min_date <= max_date:\n",
        "\n",
        "            date = (min_date).strftime(\"%Y-%m\")\n",
        "\n",
        "            df_setores = df_setores.append({\n",
        "                'Setor': setor, \n",
        "                'Datetime':date, \n",
        "                'DividendsChangeMean': df_sector[df_sector[\"Datetime\"] == date][\"DividendsChange\"].mean(), \n",
        "                'CloseChangeMean': df_sector[df_sector[\"Datetime\"] == date][\"CloseChange\"].mean(), \n",
        "                'DividendYieldChangeMean': df_sector[df_sector[\"Datetime\"] == date][\"DividendYieldChange\"].mean(),\n",
        "                'DividendsChangeMean6M': df_sector[df_sector[\"Datetime\"] == date][\"DividendsChange6M\"].mean(),\n",
        "                'CloseChangeMean6M': df_sector[df_sector[\"Datetime\"] == date][\"CloseChange6M\"].mean(),\n",
        "                'DividendYieldChangeMean6M': df_sector[df_sector[\"Datetime\"] == date][\"DividendYieldChange6M\"].mean(),\n",
        "            }, ignore_index=True)\n",
        "\n",
        "            min_date = min_date + relativedelta(months=1)\n",
        "    \n",
        "    return df_setores\n",
        "\n",
        "def get_ifix(years):\n",
        "\n",
        "    df_ifix = pd.DataFrame()\n",
        "    final_date = pd.to_datetime('today').strftime(\"%d-%m-%Y\").replace(\"-\",\"%2F\")\n",
        "    initial_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01-01\"\n",
        "\n",
        "    headers_aux = {\n",
        "        'authority':'www.infomoney.com.br',\n",
        "        'accept':'application/json, text/javascript, */*; q=0.01',\n",
        "        'accept-language':'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "        'content-type':'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "        'authority': 'www.infomoney.com.br',\n",
        "        'origin':'https://www.infomoney.com.br',\n",
        "        'referer':'https://www.infomoney.com.br/cotacoes/b3/indice/ifix/historico/',\n",
        "    }\n",
        "\n",
        "    body_aux = 'page=0&numberItems=99999&initialDate='+initial_date+'&finalDate='+final_date+'&symbol=IFIX'\n",
        "\n",
        "    response = requests.post('https://www.infomoney.com.br/wp-json/infomoney/v1/quotes/history', headers=headers_aux,  data=body_aux)\n",
        "\n",
        "    if not str(response.content) == \"b''\":\n",
        "\n",
        "        json_response = json.loads(response.content)\n",
        "\n",
        "        jobject = []\n",
        "        for obj in json_response:\n",
        "            jobject.append({\n",
        "                'data': obj[0]['display'],\n",
        "                'Close': obj[2]\n",
        "            })\n",
        "\n",
        "        df_ifix = pd.DataFrame(jobject)\n",
        "        df_ifix['Datetime'] = pd.to_datetime(df_ifix['data'], format='%d/%m/%Y')\n",
        "        df_ifix.drop(columns={'data'}, inplace = True)\n",
        "\n",
        "    return df_ifix\n",
        "\n",
        "def add_pct_changes(df_history):\n",
        "\n",
        "    # Cria o DataFrame a ser aprimorado\n",
        "    df_improved = df_history.copy()\n",
        "\n",
        "    # Remove fundos que não possuem dados do mês anterior\n",
        "    df_improved = df_improved[df_improved['Datetime'] <= last_month]\n",
        "    df_improved = df_improved.drop_duplicates().replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    # Remove fundos que não possuem pelo menos 12 registros\n",
        "    for fund in df_improved[\"Ticker\"].unique():\n",
        "        if(len(df_improved[df_improved[\"Ticker\"] == fund]) < 12):\n",
        "            df_improved = df_improved[df_improved[\"Ticker\"] != fund]\n",
        "\n",
        "    # Normaliza os dados que sofreram desdobramento\n",
        "    ajusta_desdobramento(df_improved)\n",
        "    df_improved = df_improved.replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    # Cria a coluna DividendYield\n",
        "    df_improved['DividendYield'] = round(100*df_improved['Dividends']/df_improved['Close'],6)\n",
        "\n",
        "    # Cria novas colunas contendo a variação de valores ao longo dos meses\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendsChange'] = round(df_improved[df_improved.Ticker == fundo]['Dividends'].pct_change(),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'CloseChange'] = round(df_improved[df_improved.Ticker == fundo]['Close'].pct_change(),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendYieldChange'] = round(df_improved[df_improved.Ticker == fundo]['DividendYield'].pct_change(),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendsChange6M'] = round(df_improved[df_improved.Ticker == fundo]['Dividends'].pct_change(periods=6),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'CloseChange6M'] = round(df_improved[df_improved.Ticker == fundo]['Close'].pct_change(periods=6),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendYieldChange6M'] = round(df_improved[df_improved.Ticker == fundo]['DividendYield'].pct_change(periods=6),6)\n",
        "\n",
        "    # Procura no DataFrame registros com variações muito discrepantes\n",
        "    df_improved = remove_big_variations(df_improved, {'DividendsChange': 500, 'CloseChange': 0.35})\n",
        "\n",
        "    return df_improved\n",
        "\n",
        "def improve_history(df_history, df_funds):\n",
        "\n",
        "    df_improved = add_pct_changes(df_history)\n",
        "\n",
        "    df_sectors = getSectorMeans(df_funds, df_improved)\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "    \n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Cria as colunas dos índices\n",
        "    for indice in indices:\n",
        "        df_improved[indice] = float(\"NaN\")\n",
        "                    \n",
        "    # Insere preço dos índices e a média do setor ao longo do tempo\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "\n",
        "        print(str(index+1) + \"/\" + str(len(df_improved['Ticker'].unique())))\n",
        "\n",
        "        sector = df_funds[df_funds[\"Ticker\"] == fundo][\"Setor\"].values[0]\n",
        "\n",
        "        for data in df_improved['Datetime']:\n",
        "\n",
        "            df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"IFIX\"] = get_month_close(df_ifix, data)\n",
        "\n",
        "            sector_values = df_sectors[(df_sectors[\"Datetime\"] == data) & (df_sectors[\"Setor\"] == sector)]\n",
        "\n",
        "            if len(sector_values) > 0:\n",
        "                for mean_col in [\"DividendsChangeMean\", \"CloseChangeMean\", \"DividendYieldChangeMean\"]:\n",
        "                    mean_value = sector_values[mean_col].values[0]\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"Sector\" + mean_col] = float(mean_value)\n",
        "\n",
        "            for indice in indices:\n",
        "                indice_values = df_indices[indice][df_indices[indice].Timestamp == data]['Value'].values\n",
        "                if len(indice_values) > 0:\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), indice] = float(indice_values[0])\n",
        "\n",
        "\n",
        "    df_improved['IFIX'] = df_improved['IFIX'].astype(float)\n",
        "    df_improved = remove_big_variations(df_improved, {'CloseChange': 0.35})\n",
        "\n",
        "    return df_improved"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if consume_data_from_csv:\n",
        "\n",
        "    # Os dados serão consumidos de dados previamente coletados e armazenados em GitHub\n",
        "    \n",
        "    df_funds = pd.read_csv(\"https://raw.githubusercontent.com/HpDiniz/Artigo-BWAIF-2023-Henrique-Diniz/main/df_funds_2022-12.csv\", encoding ='iso-8859-1', sep=\";\").set_index('Unnamed: 0')\n",
        "    df_funds.index.name = None\n",
        "\n",
        "    df_history = pd.read_csv(\"https://raw.githubusercontent.com/HpDiniz/Artigo-BWAIF-2023-Henrique-Diniz/main/df_history_2022-12.csv\", encoding ='iso-8859-1', sep=\";\").set_index('Unnamed: 0')\n",
        "    df_history.index.name = None\n",
        "\n",
        "    df_adress = pd.read_csv(\"https://raw.githubusercontent.com/HpDiniz/Artigo-BWAIF-2023-Henrique-Diniz/main/df_adress_2022-12.csv\", sep=\",\").set_index('Unnamed: 0')\n",
        "    df_adress.index.name = None\n",
        "\n",
        "else:\n",
        "\n",
        "    # Os dados serão coletados através de web scraping em diversas fontes distintas\n",
        "\n",
        "    df_funds = get_all_funds()\n",
        "\n",
        "    df_history, df_adress = process_history(df_funds, 2)\n",
        "\n",
        "    df_history = improve_history(df_history, df_funds)"
      ],
      "metadata": {
        "id": "vozYyokYdT3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove fundos imobiliários que possuírem 3 meses seguidos sem variação no preço da cota\n",
        "\n",
        "invalid_tickers = list(df_history[(df_history['CloseChange'] == 0.0) & (df_history['CloseChange'].shift(1) == 0.0) & (df_history['CloseChange'].shift(2) == 0.0)].Ticker.unique())\n",
        "df_history = df_history[~(df_history['Ticker'].isin(invalid_tickers))]"
      ],
      "metadata": {
        "id": "_gyGZBVLgklR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_funds.head()"
      ],
      "metadata": {
        "id": "mcWSja36gNM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_history.head()"
      ],
      "metadata": {
        "id": "6Jlh3XhpgOlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_adress.head()"
      ],
      "metadata": {
        "id": "Fe6THoP8gRON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzFenxM4xVCE"
      },
      "outputs": [],
      "source": [
        "# Exibe informações sobre os dados obtidos\n",
        "\n",
        "print(f\"{len(df_history.Ticker.unique())} FIIs restaram na análise:\")\n",
        "\n",
        "for tipo in [\"Papel\", \"Hibrido\", \"Tijolo\"]:\n",
        "    print(f\"- {len(filtra_tipo(df_history, df_funds, tipo).Ticker.unique())} FIIs de {tipo}.\")\n",
        "\n",
        "print(f\"\\n{len(df_adress)} endereços de FIIs foram encontrados.\")\n",
        "\n",
        "percent = df_adress['Latitude'].isnull().sum()/(len(df_adress))*100\n",
        "print(\"%.2f%% dos endereços estão sem Latitude e Longitude.\" % percent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHjJfYeVwdDX"
      },
      "source": [
        "# 2. Data Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiUpK1uAufh3"
      },
      "outputs": [],
      "source": [
        "def calculate_mae_rmse(abs_errors):\n",
        "\n",
        "    # Garante que teremos apenas 6 casas decimais\n",
        "    abs_errors = [round(x,6) for x in abs_errors]\n",
        "\n",
        "    # Calcular o Mean Absolute Error (MAE)\n",
        "    mae = mean_absolute_error(y_true=np.zeros_like(abs_errors), y_pred=np.array(abs_errors))\n",
        "\n",
        "    # Calcular o Root Mean Squared Error (RMSE)\n",
        "    rmse = mean_squared_error(y_true=np.zeros_like(abs_errors), y_pred=np.array(abs_errors), squared=False)\n",
        "\n",
        "    return round(mae, 6), round(rmse, 6)\n",
        "\n",
        "def upload_errors(pred_col, train_cols, strategy, sectors, abs_errors, params = None):\n",
        "\n",
        "    mae, rmse = calculate_mae_rmse(abs_errors)\n",
        "\n",
        "    with mlflow.start_run(run_name=train_cols):   \n",
        "        \n",
        "        df_abs = pd.DataFrame(abs_errors, columns = ['errors'])\n",
        "\n",
        "        # Parameters\n",
        "        mlflow.log_param(\"pred_col\", unidecode(pred_col))\n",
        "        mlflow.log_param(\"train_cols\", unidecode(train_cols))\n",
        "        mlflow.log_param(\"strategy\", unidecode(strategy))\n",
        "        mlflow.log_param(\"sector\", unidecode(sectors))\n",
        "\n",
        "        # Error Metrics\n",
        "        mlflow.log_metric(\"MAE\", round(mae, 6))\n",
        "        mlflow.log_metric(\"RMSE\", round(rmse, 6))\n",
        "        mlflow.log_metric(\"mean\", df_abs.describe().values[1][0])\n",
        "        mlflow.log_metric(\"std\", df_abs.describe().values[2][0])\n",
        "        mlflow.log_metric(\"min\", df_abs.describe().values[3][0])\n",
        "        mlflow.log_metric(\"25 pct.\", df_abs.describe().values[4][0])\n",
        "        mlflow.log_metric(\"50 pct.\", df_abs.describe().values[5][0])\n",
        "        mlflow.log_metric(\"75 pct.\", df_abs.describe().values[6][0])\n",
        "        mlflow.log_metric(\"max\", df_abs.describe().values[7][0])\n",
        "\n",
        "        # Machine Learning Params\n",
        "        mlflow.log_metric(\"n_estimators\", 0.0 if params == None else params[\"n_estimators\"]) \n",
        "        mlflow.log_metric(\"learning_rate\", 0.0 if params == None else params[\"learning_rate\"]) \n",
        "        mlflow.log_metric(\"max_depth\", 0.0 if params == None else params[\"max_depth\"]) \n",
        "        mlflow.log_metric(\"min_child_weight\", 0.0 if params == None else params[\"min_child_weight\"]) \n",
        "        mlflow.log_metric(\"colsample_bytree\", 0.0 if params == None else params[\"colsample_bytree\"])\n",
        "\n",
        "def get_experiments_result(experiment_name, sort_column = \"\"):\n",
        "\n",
        "    df = mlflow.search_runs([mlflow_experiment._experiment_id])\n",
        "\n",
        "    if(len(df) > 0):\n",
        "        if sort_column in df.columns:\n",
        "            return df.sort_values(by=sort_column, ascending=True)\n",
        "        else:\n",
        "            return df\n",
        "\n",
        "    return df\n",
        "\n",
        "def is_mlfow_configured():\n",
        "    return not (os.environ['MLFLOW_TRACKING_USERNAME'] == \"\" or os.environ['MLFLOW_TRACKING_PASSWORD'] == \"\" or os.environ['MLFLOW_TRACKING_PROJECTNAME'] == \"\")\n",
        "\n",
        "def get_possibilities(target_column, training_columns):\n",
        "\n",
        "    possibilities = []\n",
        "    for L in range(len(training_columns) + 1):\n",
        "        for subset in itertools.combinations(training_columns, L):\n",
        "            possibilities.append([target_column] + list(subset))\n",
        "\n",
        "    possibilities.reverse()\n",
        "\n",
        "    return list(filter(lambda x: len(x) > 4, possibilities)) \n",
        "\n",
        "def train_test_split(data, perc):\n",
        "\n",
        "    data = data.values\n",
        "    n = int(len(data) * (1 - perc))\n",
        "    return data[:n], data[n:]\n",
        "\n",
        "def arima_predict(df_history, fundo, pred_col, pred_index = 1):\n",
        "\n",
        "    df = df_history[df_history['Ticker'] == fundo].copy()\n",
        "    df['Target'] = df[pred_col].shift(-pred_index)\n",
        "    df = df[[pred_col, 'Target']]\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    y_pred = []\n",
        "    train, test = train_test_split(df, 0.4) # 60% de treino\n",
        "\n",
        "    # cria a variável history\n",
        "    history = [x[0] for x in train]\n",
        "\n",
        "    for i in range(len(test)):\n",
        "        test_X, test_y = test[i, :-1], test[i, -1]\n",
        "\n",
        "        # O arima deverá considerar apenas o valor da coluna principal\n",
        "        model = SARIMAX(history, order=(1,1,1), maxiter=1000)\n",
        "        resultado_sarimax = model.fit()\n",
        "\n",
        "        # Obtém a predição de {pred_index} meses à frente\n",
        "        output = resultado_sarimax.get_forecast(steps=pred_index)\n",
        "\n",
        "        # Obtém a predição do mês de interesse\n",
        "        pred = output.predicted_mean[pred_index-1]\n",
        "\n",
        "        y_pred.append(pred)\n",
        "        history.append(test[i][0])\n",
        "\n",
        "    y_real = test[:, -1]\n",
        "\n",
        "    yhat = round(y_pred[0],2)\n",
        "    abs_error = list(np.abs(y_real - y_pred))\n",
        "\n",
        "    return yhat, abs_error\n",
        "\n",
        "def model_predict(train, test_X, val_X, val_y, model):\n",
        "\n",
        "    train = np.array(train)\n",
        "    val_y = np.array([val_y])\n",
        "    val_X = np.array([val_X])\n",
        "    test_X = np.array([test_X])\n",
        "\n",
        "    X, y = train[:, :-1], train[:, -1]\n",
        "\n",
        "    if(len(val_X) > 0):\n",
        "        model.fit(X, y, eval_set=[(X,y),(val_X,val_y)], verbose=0)\n",
        "    else:\n",
        "        model.fit(X, y)\n",
        "    \n",
        "    pred = model.predict(test_X)\n",
        "\n",
        "    return pred[0]\n",
        "\n",
        "def machinelearn_predict(df_history, fundo, pred_col, train_cols, params, pred_index = 1):\n",
        " \n",
        "    df = df_history[df_history['Ticker'] == fundo][train_cols].copy()\n",
        "    df[\"Target\"] = df[pred_col].shift(-pred_index)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    model = None\n",
        "    y_pred = []\n",
        "    train, test = train_test_split(df, 0.4) # 60% de treino\n",
        "\n",
        "    history = [x for x in train]\n",
        "\n",
        "    for i in range(len(test) - 1):\n",
        "\n",
        "        val_X, val_y = test[i, :-1], test[i, -1]\n",
        "        test_X, test_y = test[i + 1, :-1], test[i + 1, -1]\n",
        "\n",
        "        model = XGBRegressor()\n",
        "        model.set_params(**params)\n",
        "\n",
        "        pred = model_predict(history, test_X, val_X, val_y, model)\n",
        "\n",
        "        y_pred.append(pred)\n",
        "\n",
        "        history.append(test[i])\n",
        "    \n",
        "    y_real = test[1:, -1]\n",
        "\n",
        "    yhat = round(y_pred[0],2)\n",
        "    abs_error = list(np.abs(y_real - y_pred))\n",
        "\n",
        "    return yhat, abs_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "existent_experiments = []\n",
        "\n",
        "# Trecho para obter os experimentos que já estão no mlflow\n",
        "if is_mlfow_configured():\n",
        "\n",
        "    mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "    mlflow_experiment = mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    try:\n",
        "        existent_experiments = list(get_experiments_result(experiment_name)[\"params.train_cols\"])\n",
        "    except:\n",
        "        print(\"Nenhum experimento foi encontrado\")"
      ],
      "metadata": {
        "id": "b1El1mOYiiBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oruj5t7-QM1B"
      },
      "outputs": [],
      "source": [
        "# Trecho para obter as combinações de colunas de acordo com a janela de treino\n",
        "pred_column = 'CloseChange'\n",
        "possibilities = get_possibilities(pred_column, (['Close','DividendYield','DividendsChange','DividendYieldChange','SectorDividendsChangeMean','SectorCloseChangeMean','Selic','IPCA','IGPM','IFIX']))\n",
        "\n",
        "if janela_treino == 6:\n",
        "    pred_column = 'CloseChange6M'\n",
        "    possibilities = get_possibilities(pred_column, (['Close','DividendYield','DividendsChange6M','DividendYieldChange6M','SectorDividendsChangeMean','SectorCloseChangeMean','Selic','IPCA','IGPM','IFIX']))\n",
        "\n",
        "print(str(len(possibilities)) + \" possibilidades serão avaliadas\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtem os Tickers de cada tipo\n",
        "papel = list(filtra_tipo(df_history, df_funds, 'Papel').Ticker.unique())\n",
        "tijolo = list(filtra_tipo(df_history, df_funds, 'Tijolo').Ticker.unique())\n",
        "hibrido = list(filtra_tipo(df_history, df_funds, 'Hibrido').Ticker.unique())"
      ],
      "metadata": {
        "id": "1qG8aJyZjTkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtra o dataframe de acordo com o tipo que se deseja treinar\n",
        "df_history = filtra_tipo(df_history, df_funds, tipo_interesse)\n",
        "sectors = pd.merge(df_funds, df_history, on='Ticker')[\"Setor\"].unique()"
      ],
      "metadata": {
        "id": "7psl0yWvFXVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHaiq-e3spev"
      },
      "outputs": [],
      "source": [
        "params = {\"n_estimators\" : 4000, \"early_stopping_rounds\" : 100, \"learning_rate\": 0.1, \"max_depth\": 6,\"min_child_weight\" : 1,\"colsample_bytree\" : 0.4}\n",
        "\n",
        "# Realiza o treino de todas as combinações com XGboost\n",
        "for j, array_possibility in enumerate(possibilities):\n",
        "    \n",
        "    abs_errors = []\n",
        "    string_possibility =  \", \".join(array_possibility)\n",
        "\n",
        "    print('Calculating \"'+ string_possibility + '\": ' + str(j+1) + '/' + str(len(possibilities)))\n",
        "\n",
        "    if string_possibility not in existent_experiments:\n",
        "\n",
        "        for i, fundo in enumerate(df_history[\"Ticker\"].unique()):\n",
        "\n",
        "            print(\"Calculating errors of \"+ fundo + \": \" + str(i+1) + \"/\" + str(len(df_history[\"Ticker\"].unique())))\n",
        "\n",
        "            prediction, abs_err = machinelearn_predict(df_history, fundo, pred_column, array_possibility, params, janela_treino)\n",
        "            abs_errors = abs_errors + abs_err\n",
        "\n",
        "        print(f'(MAE, RMSE): {calculate_mae_rmse(abs_errors)}\\n')\n",
        "\n",
        "        if is_mlfow_configured():\n",
        "            upload_errors(pred_column, string_possibility, 'Xgboost', ', '.join(sectors), abs_errors, params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abs_errors = []\n",
        "\n",
        "# Realiza o treino do ARIMA\n",
        "if pred_column not in existent_experiments:\n",
        "\n",
        "    print('Calculating ARIMA...')\n",
        "\n",
        "    for i, fundo in enumerate(df_history[\"Ticker\"].unique()):\n",
        "\n",
        "        print(\"Calculating errors of \"+ fundo + \": \" + str(i+1) + \"/\" + str(len(df_history[\"Ticker\"].unique())))\n",
        "\n",
        "        prediction, abs_err = arima_predict(df_history, fundo, pred_column, janela_treino)\n",
        "        abs_errors = abs_errors + abs_err\n",
        "\n",
        "    print(f'(MAE, RMSE): {calculate_mae_rmse(abs_errors)}\\n')\n",
        "    \n",
        "    if is_mlfow_configured():\n",
        "        upload_errors(pred_column, pred_column, 'Arima', ', '.join(sectors), abs_errors)"
      ],
      "metadata": {
        "id": "ZeK40jWB-yxD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}