{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HpDiniz/Artigo-BWAIF-2023-Henrique-Diniz/blob/main/Artigo_BWAIF_2023_Henrique_Diniz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z52BuxHJdJy-"
      },
      "outputs": [],
      "source": [
        "# Define a janela de treino em meses: 1 ou 6\n",
        "janela_treino = 1\n",
        "\n",
        "# Define qual a categoria será treinada: Geral, Papel, Tijolo ou Hibrido\n",
        "tipo_interesse = \"Hibrido\"\n",
        "\n",
        "# Define se os dados serão coletados ou obtidos de um csv armazenado no github\n",
        "consume_data_from_csv = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define as credenciais para utilização do MlFlow via DagsHub\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"\"\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"\""
      ],
      "metadata": {
        "id": "j781eHt9vAfK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiywEkwhdHss"
      },
      "source": [
        "# 0. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rFZouOfDMgM9"
      },
      "outputs": [],
      "source": [
        "import warnings;\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2I3vDXARwdDR"
      },
      "outputs": [],
      "source": [
        "!pip install pystan --quiet\n",
        "!pip install statsmodels --quiet\n",
        "!pip install xgboost==1.6.2 --quiet\n",
        "!pip install pmdarima --quiet\n",
        "!pip install mysqlclient --quiet\n",
        "!pip install psycopg2-binary==2.8.6 --quiet\n",
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet\n",
        "!pip install unidecode --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZDko7zAeR0zL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "import json\n",
        "import mlflow\n",
        "import requests\n",
        "import itertools\n",
        "\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import date\n",
        "from getpass import getpass\n",
        "from bs4 import BeautifulSoup\n",
        "from unidecode import unidecode\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rsm__ClwdDS"
      },
      "source": [
        "# 1. Read in Data and Process Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yyp-wBpT4wr-"
      },
      "outputs": [],
      "source": [
        "this_month = \"2023-01\"\n",
        "last_month = \"2022-12\"\n",
        "\n",
        "headers = {\n",
        "    'User-Agent':\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36'\n",
        "        ' (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'\n",
        "}\n",
        "\n",
        "experiment_name = f'{last_month}-{tipo_interesse}-{janela_treino}M'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dsgUw0m7Xw0q"
      },
      "outputs": [],
      "source": [
        "def converteData(datas, monthYearOnly):\n",
        "\n",
        "    new_array = []\n",
        "    meses = [\"Janeiro\",\"Fevereiro\",\"Março\",\"Abril\",\"Maio\",\"Junho\",\"Julho\",\"Agosto\",\"Setembro\",\"Outubro\",\"Novembro\",\"Dezembro\"]\n",
        "\n",
        "    for data in datas:\n",
        "\n",
        "        item = data.split(\"/\")\n",
        "        mes = str(meses.index(item[0])+1)\n",
        "        mes = (\"0\" + mes)[len(mes)-1:len(mes)+1]\n",
        "\n",
        "        new_date = item[1] + \"-\" + mes\n",
        "\n",
        "        if not monthYearOnly:\n",
        "            new_date = new_date + \"-01 00:00:00\"\n",
        "        \n",
        "        new_array.append(new_date)\n",
        "        \n",
        "    return new_array\n",
        "\n",
        "def obtem_datas_faltantes(df, date_colun):\n",
        "\n",
        "    datas_faltantes = []\n",
        "    start_date = df[date_colun].min()\n",
        "    end_date = df[date_colun].max()\n",
        "\n",
        "    while(start_date < end_date):\n",
        "        date = str(start_date)[0:10]\n",
        "        df_aux = df[df[date_colun] == date]\n",
        "\n",
        "        if(len(df_aux) < 1):\n",
        "            datas_faltantes.append(date)\n",
        "\n",
        "        start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    return datas_faltantes\n",
        "\n",
        "def obtem_dados_mercado(indice):\n",
        "\n",
        "    indice = indice.lower()\n",
        "\n",
        "    if indice == \"igpm\":\n",
        "        indice = \"igp-m\"\n",
        "\n",
        "    response = requests.get('https://www.dadosdemercado.com.br/economia/' + indice, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df_igpm = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    anos = list(df_igpm.iloc[:, 0].values)\n",
        "\n",
        "    timestamp = []\n",
        "    values = []\n",
        "\n",
        "    for i in range(len(anos)):\n",
        "        for m in range(12, 0, -1):\n",
        "            taxa = str(list(df_igpm.iloc[:, m].values)[i])\n",
        "            if taxa != '--':\n",
        "                mes = str(m) if m > 9 else \"0\" + str(m)\n",
        "                timestamp.append(str(anos[i]) + \"-\" + mes)\n",
        "                values.append(round(float(taxa.replace(\"%\",\"\").replace(\",\",\".\")), 2))\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_tax = pd.DataFrame({\n",
        "        'Timestamp': timestamp,\n",
        "        'Value': values\n",
        "    })\n",
        "\n",
        "    df_tax['Value'] = pd.to_numeric(df_tax['Value'], downcast=\"float\")\n",
        "\n",
        "    return df_tax.replace(0, 0.01) \n",
        "\n",
        "def get_all_funds():\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/ranking', headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        df = pd.read_html(response.content, encoding='utf-8')[0]\n",
        "\n",
        "    idx = df[df['Setor'].isna()].index\n",
        "    df_funds = df.drop(idx)\n",
        "\n",
        "    df_funds = df_funds.rename(columns=lambda x: re.sub(r'Código\\s*do\\s*fundo', 'Ticker', x))\n",
        "    df_funds = df_funds.rename(columns=lambda x: re.sub(r'Quantidade\\s*Ativos', 'QuantidadeAtivos', x))\n",
        "\n",
        "    col_categorical = ['Ticker','Setor']\n",
        "    df_funds[col_categorical] = df_funds[col_categorical].astype('category')\n",
        "\n",
        "    df_funds.sort_values('Ticker', inplace=True)\n",
        "\n",
        "    df_funds = df_funds.drop_duplicates(subset=['Ticker']).replace('Títulos e Valores Mobiliários','Títulos e Val. Mob.')\n",
        "\n",
        "    df_funds = df_funds[['Ticker','Setor','QuantidadeAtivos']].reset_index(drop=True)\n",
        "\n",
        "    return df_funds\n",
        "\n",
        "def get_close(fund, years):\n",
        "\n",
        "    df_close = pd.DataFrame()\n",
        "\n",
        "    start_date = \"01-01-\" + str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) \n",
        "    end_date = pd.to_datetime('today').replace(day=1,hour=0,minute=0,second=0,microsecond=0).strftime(\"%d-%m-%Y\")\n",
        "\n",
        "    response = requests.get('https://fii-api.infomoney.com.br/api/v1/fii/cotacao/historico/grafico?Ticker='+fund+'&DataInicio='+start_date+'&DataFim='+end_date, headers=headers)\n",
        "\n",
        "    if not str(response.content) == \"b''\":\n",
        "\n",
        "        json_response = json.loads(response.content)\n",
        "\n",
        "        if 'errors' in json_response:\n",
        "            print(str(json_response['errors']))\n",
        "        else:\n",
        "            df_close = pd.read_json(json.dumps(json_response['dataValor']))\n",
        "\n",
        "            df_close['Ticker'] = fund\n",
        "            df_close['Ticker'] = df_close['Ticker'].astype('category')\n",
        "\n",
        "            df_close.rename(columns={'valor': 'Close'}, inplace = True)\n",
        "\n",
        "            df_close['Datetime'] = pd.to_datetime(df_close['data'], format='%d-%m-%YT%H:%M:%S')\n",
        "\n",
        "            df_close.drop(columns={'data'}, inplace = True)\n",
        "        \n",
        "    return df_close.replace(0, 0.01) \n",
        "\n",
        "def get_dividends(fund, years):\n",
        "\n",
        "    min_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01\"\n",
        "\n",
        "    response = requests.get('https://www.fundsexplorer.com.br/funds/' + fund, headers=headers)\n",
        "\n",
        "    soup = bs4.BeautifulSoup(response.content, \"html\")\n",
        "    div = soup.find(\"div\", {\"id\": \"dividends-chart-wrapper\"})\n",
        "\n",
        "    labels = re.findall('\"labels\":\\[.*?\\]', str(div))\n",
        "    dividends = re.findall('\"data\":\\[.*?\\]', str(div))\n",
        "\n",
        "    dividends = json.loads(\"{\" + dividends[0] + \"}\")['data']\n",
        "    labels = json.loads(\"{\" + labels[0] + \"}\")['labels']\n",
        "\n",
        "    dates = converteData(labels, True)\n",
        "\n",
        "    result = []\n",
        "    if len(dates) > 0 and len(dates) == len(dividends):\n",
        "        for i in range(len(dates)):\n",
        "            if dates[i] >= min_date:\n",
        "                result.append({\n",
        "                    \"Ticker\": fund,\n",
        "                    \"Datetime\": dates[i],\n",
        "                    \"Dividends\": round(dividends[i],2)\n",
        "                })\n",
        "\n",
        "    df_dividends = pd.DataFrame(result)\n",
        "\n",
        "    return df_dividends.replace(0, 0.01) \n",
        "\n",
        "def get_adress(fundo):\n",
        "\n",
        "    api_url = \"https://fii-api.infomoney.com.br/api/v1/propertie/\" + fundo\n",
        "    response = requests.get(api_url)\n",
        "    data = []\n",
        "\n",
        "    if '{' in str(response.content):\n",
        "\n",
        "        response = response.json()\n",
        "\n",
        "        for item in response[\"property\"]:\n",
        "\n",
        "            row = {\n",
        "                \"Ticker\": fundo,\n",
        "                \"Tipo\": item[\"type\"],\n",
        "                \"Nome\": item[\"name\"],\n",
        "                \"DataCompra\": item[\"datePurchase\"],\n",
        "                \"ValorAreaBrutaLocavel\": item[\"valueGrossLeasableArea\"],\n",
        "                \"Estado\": item[\"state\"],\n",
        "                \"Cidade\": item[\"city\"],\n",
        "                \"Endereco\": item[\"address\"],\n",
        "                \"GoogleMapsLink\": item[\"googleMapsLink\"],\n",
        "                \"PercentualPartic\": item[\"percentagePartic\"],\n",
        "                \"PecentualVacancia\": item[\"percentVacancy\"],\n",
        "                \"PercentualInadimplencia90Dias\": item[\"percent90DayDeliquency\"],\n",
        "                \"PercentualFii\": item[\"percentFii\"],\n",
        "                \"Latitude\": float(\"NaN\"),\n",
        "                \"Longitude\": float(\"NaN\")\n",
        "            }\n",
        "\n",
        "            cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", item['googleMapsLink'])\n",
        "\n",
        "            if(len(cordinates) > 0):\n",
        "                cordinates = cordinates[0].split(\",\")\n",
        "                row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "            else:\n",
        "                \n",
        "                adress_url = (\"https://www.google.com/maps/place/\" + item[\"address\"] + \",\" + item[\"city\"] + \"-\" + item[\"state\"]).replace(\" \", \"%20\")\n",
        "\n",
        "                response = requests.get(adress_url)\n",
        "\n",
        "                cordinates = re.findall(\"(?<=@)[-]*[\\d.]*,-[\\d.]*\", str(response.content))\n",
        "\n",
        "                if(len(cordinates) > 0):\n",
        "                    print(\"Endereço não encontrado, obtendo Latitude e Longitude aproximada...\")\n",
        "                    cordinates = cordinates[0].split(\",\")\n",
        "                    row[\"Latitude\"], row[\"Longitude\"] = float(cordinates[0]), float(cordinates[1])\n",
        "                else:\n",
        "                    print(\"Endereço não encontrado e FALHA ao obter Latitude e Longitude aproximada...\")\n",
        "\n",
        "            data.append(row)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def filtra_tipo(df_history, df_funds, tipo):\n",
        "\n",
        "    tickers = list(df_funds['Ticker'].values)\n",
        "\n",
        "    if tipo == \"Papel\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] == \"Títulos e Val. Mob.\")]['Ticker'].values)\n",
        "    elif tipo == \"Tijolo\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] != \"Títulos e Val. Mob.\") & (df_funds[\"Setor\"] != \"Híbrido\")]['Ticker'].values)\n",
        "    elif tipo == \"Hibrido\":\n",
        "        tickers = list(df_funds[(df_funds[\"Setor\"] == \"Híbrido\")]['Ticker'].values)\n",
        "    \n",
        "    return df_history[df_history['Ticker'].isin(tickers)]\n",
        "\n",
        "def get_month_close(df_close, date):\n",
        "\n",
        "    year = int(date.split('-')[0])\n",
        "    month = int(date.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month= month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "    end_date = (start_date + relativedelta(months=1))\n",
        "\n",
        "    df_aux = df_close.copy()\n",
        "\n",
        "    df_aux = df_aux[df_aux['Datetime'] >= start_date]\n",
        "    df_aux = df_aux[df_aux['Datetime'] < end_date]\n",
        "\n",
        "    if len(df_aux) > 0:\n",
        "        return float(df_aux.values[-1][0])\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def has_missing_data(df_history):\n",
        "\n",
        "    min = str(df_history['Datetime'].min())\n",
        "    max = str(df_history['Datetime'].max())\n",
        "\n",
        "    year = int(max.split('-')[0])\n",
        "    month = int(max.split('-')[1])\n",
        "\n",
        "    start_date = pd.to_datetime('today').replace(year=year, month=month, day=1,hour=0,minute=0,second=0,microsecond=0)\n",
        "\n",
        "    while str(start_date.strftime(\"%Y-%m\")) != min:\n",
        "\n",
        "        if not str(start_date.strftime(\"%Y-%m\")) in list(df_history['Datetime']):\n",
        "            return True\n",
        "\n",
        "        start_date = (start_date - relativedelta(months=1))\n",
        "\n",
        "    return False\n",
        "\n",
        "def get_history(fund, years):\n",
        "\n",
        "    df_close = get_close(fund, years)\n",
        "    df_dividends = get_dividends(fund, years)\n",
        "\n",
        "    df_history = df_dividends.copy()\n",
        "\n",
        "    if len(df_history) > 0 and len(df_close) > 0:\n",
        "\n",
        "        new_df = []\n",
        "        for index, row in df_history.iterrows():\n",
        "\n",
        "            #print(\"Procurando 'Close' de: \" + row['Datetime'])\n",
        "            row['Dividends'] = round(row['Dividends'],2)\n",
        "            row['Close'] = get_month_close(df_close, row['Datetime'])\n",
        "            new_df.append(row)\n",
        "\n",
        "        df_history = pd.DataFrame(new_df)\n",
        "\n",
        "        datas = list(df_history['Datetime'])\n",
        "\n",
        "        if has_missing_data(df_history):\n",
        "            print(\"FII \" + fund + \" será removido por estar com dados faltantes.\")\n",
        "            df_history = pd.DataFrame()\n",
        "    \n",
        "    return df_history\n",
        "\n",
        "def process_daily_history(df_history, years):\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "\n",
        "    # Cria o histórico diário\n",
        "    df_history_daily = pd.DataFrame()\n",
        "\n",
        "    for fund in df_history['Ticker'].unique():\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_close = get_close(fund, years)\n",
        "\n",
        "        df_close[\"Datetime\"] = pd.to_datetime(df_close[\"Datetime\"], format=\"%Y-%m-%d\")\n",
        "\n",
        "        # Preenche os índices mensais\n",
        "        meses_percorridos = []\n",
        "\n",
        "        for index, row in df_close.iterrows():\n",
        "            \n",
        "            data_mes = str(row['Datetime'])[0:7]\n",
        "            df_aux = df_history[(df_history['Datetime'] == data_mes) & (df_history['Ticker'] == fund)]\n",
        "\n",
        "            if len(df_aux) < 1 or data_mes in meses_percorridos:\n",
        "                continue\n",
        "\n",
        "            meses_percorridos.append(data_mes)\n",
        "            df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), \"Dividends\"] = float(df_aux['Dividends'].values[0])\n",
        "            \n",
        "            for indice in indices:\n",
        "                df_aux = df_indices[indice][df_indices[indice]['Timestamp'] == data_mes]\n",
        "                df_close.loc[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes)), indice] = float(df_aux['Value'].values[0])\n",
        "            \n",
        "            df_history_daily = df_history_daily.append(df_close[(df_close['Ticker'] == fund) & (df_close[\"Datetime\"].dt.strftime(\"%Y-%m\").eq(data_mes))])\n",
        "\n",
        "    # Preenche o IFIX em todas as datas do histórico diário\n",
        "    datas_percorridos = []\n",
        "    for index, row in df_history_daily.iterrows():\n",
        "        \n",
        "        data = str(row[\"Datetime\"])[0:10]\n",
        "\n",
        "        print(\"Preenchendo IFIX em \" + data + \"...\")\n",
        "\n",
        "        if data not in datas_percorridos:\n",
        "\n",
        "            df_aux = df_ifix[df_ifix['Datetime'] == data]\n",
        "\n",
        "            if(len(df_aux) > 0):\n",
        "\n",
        "                df_history_daily.loc[(df_history_daily[\"Datetime\"].dt.strftime(\"%Y-%m-%d\").eq(data)), \"IFIX\"] = float(df_aux['Close'].values[0])\n",
        "                datas_percorridos.append(data)\n",
        "\n",
        "    return df_history_daily\n",
        "\n",
        "def preenche_historico_faltante(df_history_daily):\n",
        "\n",
        "    # Percorre todos os ativos do histórico\n",
        "    for ticker in df_history_daily['Ticker'].unique():\n",
        "\n",
        "        print(\"Adicionando dados faltantes de \" + ticker + \"...\")\n",
        "\n",
        "        # Obtém o histórico específico do ativo\n",
        "        df_aux = df_history_daily[df_history_daily['Ticker'] == ticker].copy()\n",
        "\n",
        "        # Obtém a menor e a maior data do histórico do ativo\n",
        "        start_date = pd.to_datetime(df_aux['Datetime']).min() + relativedelta(days=1)\n",
        "        end_date = pd.to_datetime(df_aux['Datetime']).max()\n",
        "\n",
        "        # Percorra todas as datas do intervalo\n",
        "        while(start_date < end_date):\n",
        "            \n",
        "            # Caso não haja algum registro no histórico para a data atual...\n",
        "            if (len(df_aux[df_aux['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(start_date)[0:10])]) < 1):\n",
        "                \n",
        "                # Obtém a data de ontém\n",
        "                ontem = (start_date - relativedelta(days=1))\n",
        "\n",
        "                # Obtém os registros de ontém\n",
        "                df_ontem = df_history_daily[(df_history_daily['Ticker'] == ticker) & (df_history_daily['Datetime'].dt.strftime(\"%Y-%m-%d\").eq(str(ontem)[0:10]))]\n",
        "                \n",
        "                # Adiciona a data faltante no histórico\n",
        "                df_history_daily = df_history_daily.append(pd.DataFrame({\n",
        "                    \"Close\": df_ontem['Close'].values[0],\n",
        "                    \"Dividends\": df_ontem['Dividends'].values[0],\n",
        "                    \"Ticker\": [ticker],\n",
        "                    \"Datetime\": [start_date],\n",
        "                    \"Selic\": df_ontem['Selic'].values[0],\n",
        "                    \"IPCA\": df_ontem['IPCA'].values[0],\n",
        "                    \"IGPM\": df_ontem['IGPM'].values[0],\n",
        "                    \"IFIX\": df_ontem['IFIX'].values[0]\n",
        "                }))\n",
        "\n",
        "            # Incrementa a data de início\n",
        "            start_date = (start_date + relativedelta(days=1))\n",
        "\n",
        "    # Ordena todos os registros pelo Ticker e Data\n",
        "    df_history_daily.sort_values(by=['Ticker', 'Datetime'], inplace = True)\n",
        "    df_history_daily = df_history_daily.reset_index(drop = True)\n",
        "    return df_history_daily\n",
        "\n",
        "def process_history(df_funds, years):\n",
        "\n",
        "    df_adress = pd.DataFrame()\n",
        "    df_history = pd.DataFrame()\n",
        "    \n",
        "    # Percorre a lista de fundos para obter o histórico individual de cada um deles\n",
        "    for fund in df_funds['Ticker']:\n",
        "\n",
        "        print(\"Coletando informações de \" + fund + \"...\")\n",
        "\n",
        "        df_aux_1 = get_adress(fund)\n",
        "        df_aux_2 = get_history(fund, years)\n",
        "        \n",
        "        df_adress = df_adress.append(df_aux_1)\n",
        "        df_history = df_history.append(df_aux_2)\n",
        "\n",
        "        print(str(len(df_aux_2)) + \" dados de histórico e \" + str(len(df_aux_1)) + \" endereços foram encontrados.\")\n",
        "\n",
        "    is_NaN = df_history.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis=1)\n",
        "    rows_with_NaN = df_history[row_has_NaN]\n",
        "    tickers = rows_with_NaN['Ticker'].unique()\n",
        "    df_history = df_history[~df_history['Ticker'].isin(tickers)]\n",
        "\n",
        "    df_history = df_history[df_history['Datetime'] <= last_month]\n",
        "    df_history = df_history.drop_duplicates().replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    for fund in df_history[\"Ticker\"].unique():\n",
        "        if(len(df_history[df_history[\"Ticker\"] == fund]) < 12):\n",
        "            df_history = df_history[df_history[\"Ticker\"] != fund]\n",
        "\n",
        "    a = df_history[df_history['Datetime'] == last_month].Ticker.values\n",
        "    b = df_history.Ticker.unique()\n",
        "    intersection = list(set(a) & set(b))\n",
        "    fundos_faltantes = list(set(a) ^ set(b))\n",
        "\n",
        "    df_history = df_history[~df_history['Ticker'].isin(fundos_faltantes)]\n",
        "    \n",
        "    return df_history, df_adress\n",
        "\n",
        "def remove_big_variations(df, col_dict):\n",
        "\n",
        "    drop_indexes = []\n",
        "    for index, fundo in enumerate(df['Ticker'].unique()):\n",
        "        df_variacoes = df[df[\"Ticker\"] == fundo]\n",
        "\n",
        "        for key in col_dict:\n",
        "            df_variacoes = df_variacoes[(abs(df[key]) >= col_dict[key])]\n",
        "\n",
        "            if len(df_variacoes) > 0:\n",
        "                drop_indexes = drop_indexes + list(df[(df[\"Datetime\"] <= df_variacoes[\"Datetime\"].values[-1]) & (df.Ticker == fundo)].index)\n",
        "\n",
        "    df = df.drop(drop_indexes)\n",
        "    ticker_before = df[\"Ticker\"].unique()\n",
        "\n",
        "    # Remove fundos que não possuem pelo menos 20 registros\n",
        "    for fund in df[\"Ticker\"].unique():\n",
        "        if(len(df[df[\"Ticker\"] == fund]) < 20):\n",
        "            df = df[df[\"Ticker\"] != fund]\n",
        "\n",
        "    print(\"Tickers removidos: \", set(ticker_before) - set(df[\"Ticker\"].unique()))\n",
        "\n",
        "    return df\n",
        "\n",
        "def ajusta_desdobramento(df):\n",
        "    \n",
        "    # Desdobramentos obtidos em: https://br.investing.com/stock-split-calendar/\n",
        "    desdobramentos = {\n",
        "        \"BTCI11\": [\"2023-01\", 9],\n",
        "        \"CYCR11\": [\"2022-10\", 10],\n",
        "        \"EQIR11\": [\"2022-09\", 10],\n",
        "        \"VGIR11\": [\"2022-09\", 10],\n",
        "        \"GALG11\": [\"2022-08\", 10],\n",
        "        \"ARRI11\": [\"2022-08\", 10],\n",
        "        \"VIUR11\": [\"2022-05\", 10],\n",
        "        \"XPSF11\": [\"2022-05\", 10],\n",
        "        \"VIFI11\": [\"2022-04\", 10],\n",
        "        \"GAME11\": [\"2022-03\", 10],\n",
        "        \"BLMR11\": [\"2021-09\", 10],\n",
        "        \"MAXR11\": [\"2021-04\", 19],\n",
        "        \"RMAI11\": [\"2021-03\", 10],\n",
        "        \"FISC11\": [\"2020-12\", 10],\n",
        "        \"PQAG11\": [\"2020-11\", 10]\n",
        "    }\n",
        "\n",
        "    for key in desdobramentos:\n",
        "        if len(df[df[\"Ticker\"] == key]) > 0:\n",
        "            for index, row in df.iterrows():\n",
        "                if row[\"Ticker\"] == key and row[\"Datetime\"] < desdobramentos[key][0]:\n",
        "                    df.at[index,'Close'] = round(row['Close']/ desdobramentos[key][1],2)\n",
        "\n",
        "def getSectorMeans(df_funds, df_history):\n",
        "\n",
        "    df_setores = pd.DataFrame(({\n",
        "        'Setor':[],\n",
        "        'Datetime':[],\n",
        "        'DividendsChangeMean' :[],\n",
        "        'CloseChangeMean':[],\n",
        "        'DividendYieldChangeMean':[],\n",
        "        'DividendsChangeMean6M' :[],\n",
        "        'CloseChangeMean6M':[],\n",
        "        'DividendYieldChangeMean6M':[]\n",
        "    }))\n",
        "\n",
        "    for setor in df_funds[\"Setor\"].unique():\n",
        "\n",
        "        setor_tickers = df_funds[df_funds[\"Setor\"] == setor][\"Ticker\"].values\n",
        "\n",
        "        df_sector = df_history[df_history[\"Ticker\"].isin(setor_tickers)]\n",
        "        min_date = pd.to_datetime(df_sector[\"Datetime\"].min()).replace(day=1)\n",
        "        max_date = pd.to_datetime(df_sector[\"Datetime\"].max()).replace(day=1)\n",
        "\n",
        "        while min_date <= max_date:\n",
        "\n",
        "            date = (min_date).strftime(\"%Y-%m\")\n",
        "\n",
        "            df_setores = df_setores.append({\n",
        "                'Setor': setor, \n",
        "                'Datetime':date, \n",
        "                'DividendsChangeMean': df_sector[df_sector[\"Datetime\"] == date][\"DividendsChange\"].mean(), \n",
        "                'CloseChangeMean': df_sector[df_sector[\"Datetime\"] == date][\"CloseChange\"].mean(), \n",
        "                'DividendYieldChangeMean': df_sector[df_sector[\"Datetime\"] == date][\"DividendYieldChange\"].mean(),\n",
        "                'DividendsChangeMean6M': df_sector[df_sector[\"Datetime\"] == date][\"DividendsChange6M\"].mean(),\n",
        "                'CloseChangeMean6M': df_sector[df_sector[\"Datetime\"] == date][\"CloseChange6M\"].mean(),\n",
        "                'DividendYieldChangeMean6M': df_sector[df_sector[\"Datetime\"] == date][\"DividendYieldChange6M\"].mean(),\n",
        "            }, ignore_index=True)\n",
        "\n",
        "            min_date = min_date + relativedelta(months=1)\n",
        "    \n",
        "    return df_setores\n",
        "\n",
        "def get_ifix(years):\n",
        "\n",
        "    df_ifix = pd.DataFrame()\n",
        "    final_date = pd.to_datetime('today').strftime(\"%d-%m-%Y\").replace(\"-\",\"%2F\")\n",
        "    initial_date = str(int(pd.to_datetime('today').strftime(\"%Y\")) - years) + \"-01-01\"\n",
        "\n",
        "    headers_aux = {\n",
        "        'authority':'www.infomoney.com.br',\n",
        "        'accept':'application/json, text/javascript, */*; q=0.01',\n",
        "        'accept-language':'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "        'content-type':'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "        'authority': 'www.infomoney.com.br',\n",
        "        'origin':'https://www.infomoney.com.br',\n",
        "        'referer':'https://www.infomoney.com.br/cotacoes/b3/indice/ifix/historico/',\n",
        "    }\n",
        "\n",
        "    body_aux = 'page=0&numberItems=99999&initialDate='+initial_date+'&finalDate='+final_date+'&symbol=IFIX'\n",
        "\n",
        "    response = requests.post('https://www.infomoney.com.br/wp-json/infomoney/v1/quotes/history', headers=headers_aux,  data=body_aux)\n",
        "\n",
        "    if not str(response.content) == \"b''\":\n",
        "\n",
        "        json_response = json.loads(response.content)\n",
        "\n",
        "        jobject = []\n",
        "        for obj in json_response:\n",
        "            jobject.append({\n",
        "                'data': obj[0]['display'],\n",
        "                'Close': obj[2]\n",
        "            })\n",
        "\n",
        "        df_ifix = pd.DataFrame(jobject)\n",
        "        df_ifix['Datetime'] = pd.to_datetime(df_ifix['data'], format='%d/%m/%Y')\n",
        "        df_ifix.drop(columns={'data'}, inplace = True)\n",
        "\n",
        "    return df_ifix\n",
        "\n",
        "def add_pct_changes(df_history):\n",
        "\n",
        "    # Cria o DataFrame a ser aprimorado\n",
        "    df_improved = df_history.copy()\n",
        "\n",
        "    # Remove fundos que não possuem dados do mês anterior\n",
        "    df_improved = df_improved[df_improved['Datetime'] <= last_month]\n",
        "    df_improved = df_improved.drop_duplicates().replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    # Remove fundos que não possuem pelo menos 12 registros\n",
        "    for fund in df_improved[\"Ticker\"].unique():\n",
        "        if(len(df_improved[df_improved[\"Ticker\"] == fund]) < 12):\n",
        "            df_improved = df_improved[df_improved[\"Ticker\"] != fund]\n",
        "\n",
        "    # Normaliza os dados que sofreram desdobramento\n",
        "    ajusta_desdobramento(df_improved)\n",
        "    df_improved = df_improved.replace(np.inf, 0).replace(-np.inf,0).replace(0,0.001)\n",
        "\n",
        "    # Cria a coluna DividendYield\n",
        "    df_improved['DividendYield'] = round(100*df_improved['Dividends']/df_improved['Close'],6)\n",
        "\n",
        "    # Cria novas colunas contendo a variação de valores ao longo dos meses\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendsChange'] = round(df_improved[df_improved.Ticker == fundo]['Dividends'].pct_change(),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'CloseChange'] = round(df_improved[df_improved.Ticker == fundo]['Close'].pct_change(),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendYieldChange'] = round(df_improved[df_improved.Ticker == fundo]['DividendYield'].pct_change(),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendsChange6M'] = round(df_improved[df_improved.Ticker == fundo]['Dividends'].pct_change(periods=6),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'CloseChange6M'] = round(df_improved[df_improved.Ticker == fundo]['Close'].pct_change(periods=6),6)\n",
        "        df_improved.loc[df_improved.Ticker == fundo, 'DividendYieldChange6M'] = round(df_improved[df_improved.Ticker == fundo]['DividendYield'].pct_change(periods=6),6)\n",
        "\n",
        "    # Procura no DataFrame registros com variações muito discrepantes\n",
        "    df_improved = remove_big_variations(df_improved, {'DividendsChange': 500, 'CloseChange': 0.35})\n",
        "\n",
        "    return df_improved\n",
        "\n",
        "def improve_history(df_history, df_funds):\n",
        "\n",
        "    df_improved = add_pct_changes(df_history)\n",
        "\n",
        "    df_sectors = getSectorMeans(df_funds, df_improved)\n",
        "\n",
        "    # Cria um array de índices\n",
        "    indices = ['Selic','IPCA','IGPM']\n",
        "\n",
        "    # Obtém o histórico do IFIX\n",
        "    df_ifix = get_ifix(2)\n",
        "    \n",
        "    # Obtém o histórico de índices\n",
        "    df_indices = {}\n",
        "    for indice in indices:\n",
        "        df_indices[indice] = obtem_dados_mercado(indice)\n",
        "\n",
        "    # Cria as colunas dos índices\n",
        "    for indice in indices:\n",
        "        df_improved[indice] = float(\"NaN\")\n",
        "                    \n",
        "    # Insere preço dos índices e a média do setor ao longo do tempo\n",
        "    for index, fundo in enumerate(df_improved['Ticker'].unique()):\n",
        "\n",
        "        print(str(index+1) + \"/\" + str(len(df_improved['Ticker'].unique())))\n",
        "\n",
        "        sector = df_funds[df_funds[\"Ticker\"] == fundo][\"Setor\"].values[0]\n",
        "\n",
        "        for data in df_improved['Datetime']:\n",
        "\n",
        "            df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"IFIX\"] = get_month_close(df_ifix, data)\n",
        "\n",
        "            sector_values = df_sectors[(df_sectors[\"Datetime\"] == data) & (df_sectors[\"Setor\"] == sector)]\n",
        "\n",
        "            if len(sector_values) > 0:\n",
        "                for mean_col in [\"DividendsChangeMean\", \"CloseChangeMean\", \"DividendYieldChangeMean\"]:\n",
        "                    mean_value = sector_values[mean_col].values[0]\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), \"Sector\" + mean_col] = float(mean_value)\n",
        "\n",
        "            for indice in indices:\n",
        "                indice_values = df_indices[indice][df_indices[indice].Timestamp == data]['Value'].values\n",
        "                if len(indice_values) > 0:\n",
        "                    df_improved.loc[(df_improved.Ticker == fundo) & (df_improved.Datetime == data), indice] = float(indice_values[0])\n",
        "\n",
        "\n",
        "    df_improved['IFIX'] = df_improved['IFIX'].astype(float)\n",
        "    df_improved = remove_big_variations(df_improved, {'CloseChange': 0.35})\n",
        "\n",
        "    return df_improved"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if consume_data_from_csv:\n",
        "\n",
        "    # Os dados serão consumidos de dados previamente coletados e armazenados em GitHub\n",
        "    \n",
        "    df_funds = pd.read_csv(\"https://raw.githubusercontent.com/HpDiniz/Artigo-BWAIF-2023-Henrique-Diniz/main/df_funds_2022-12.csv\", encoding ='iso-8859-1', sep=\";\").set_index('Unnamed: 0')\n",
        "    df_funds.index.name = None\n",
        "\n",
        "    df_history = pd.read_csv(\"https://raw.githubusercontent.com/HpDiniz/Artigo-BWAIF-2023-Henrique-Diniz/main/df_history_2022-12.csv\", encoding ='iso-8859-1', sep=\";\").set_index('Unnamed: 0')\n",
        "    df_history.index.name = None\n",
        "\n",
        "    df_adress = pd.read_csv(\"https://raw.githubusercontent.com/HpDiniz/Artigo-BWAIF-2023-Henrique-Diniz/main/df_adress_2022-12.csv\", sep=\",\").set_index('Unnamed: 0')\n",
        "    df_adress.index.name = None\n",
        "\n",
        "else:\n",
        "\n",
        "    # Os dados serão coletados através de web scraping em diversas fontes distintas\n",
        "\n",
        "    df_funds = get_all_funds()\n",
        "\n",
        "    df_history, df_adress = process_history(df_funds, 2)\n",
        "\n",
        "    df_history = improve_history(df_history, df_funds)"
      ],
      "metadata": {
        "id": "vozYyokYdT3k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove fundos imobiliários que possuírem 3 meses seguidos sem variação no preço da cota\n",
        "\n",
        "invalid_tickers = list(df_history[(df_history['CloseChange'] == 0.0) & (df_history['CloseChange'].shift(1) == 0.0) & (df_history['CloseChange'].shift(2) == 0.0)].Ticker.unique())\n",
        "df_history = df_history[~(df_history['Ticker'].isin(invalid_tickers))]"
      ],
      "metadata": {
        "id": "_gyGZBVLgklR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_funds.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mcWSja36gNM_",
        "outputId": "ada9af70-a16c-4305-9fb7-83e7d842fb00"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ticker                Setor  QuantidadeAtivos\n",
              "0  ABCP11            Shoppings                 1\n",
              "1  AFHI11  Títulos e Val. Mob.                 0\n",
              "2  AIEC11   Lajes Corporativas                 2\n",
              "3  ALMI11   Lajes Corporativas                 1\n",
              "4  ALZM11  Títulos e Val. Mob.                 0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a01fa940-1d11-49a5-81aa-e45e6edb4147\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Setor</th>\n",
              "      <th>QuantidadeAtivos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABCP11</td>\n",
              "      <td>Shoppings</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AFHI11</td>\n",
              "      <td>Títulos e Val. Mob.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AIEC11</td>\n",
              "      <td>Lajes Corporativas</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ALMI11</td>\n",
              "      <td>Lajes Corporativas</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ALZM11</td>\n",
              "      <td>Títulos e Val. Mob.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a01fa940-1d11-49a5-81aa-e45e6edb4147')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a01fa940-1d11-49a5-81aa-e45e6edb4147 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a01fa940-1d11-49a5-81aa-e45e6edb4147');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_history.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "6Jlh3XhpgOlf",
        "outputId": "61053d34-8f53-4dbf-90d6-f394c770b90d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ticker Datetime  Dividends  Close  DividendYield  DividendsChange  \\\n",
              "0  AFHI11  2021-05       0.60  91.30       0.657174         0.200000   \n",
              "1  AFHI11  2021-06       0.90  90.30       0.996678         0.500000   \n",
              "2  AFHI11  2021-07       0.90  93.58       0.961744         0.000000   \n",
              "3  AFHI11  2021-08       1.00  93.30       1.071811         0.111111   \n",
              "4  AFHI11  2021-09       1.05  92.29       1.137718         0.050000   \n",
              "\n",
              "   CloseChange  DividendYieldChange  DividendsChange6M  CloseChange6M  \\\n",
              "0    -0.085995             0.312901                NaN            NaN   \n",
              "1    -0.010953             0.516612                NaN            NaN   \n",
              "2     0.036323            -0.035050                NaN            NaN   \n",
              "3    -0.002992             0.114445                NaN            NaN   \n",
              "4    -0.010825             0.061491                NaN            NaN   \n",
              "\n",
              "   DividendYieldChange6M  Selic  IPCA  IGPM   IFIX  SectorDividendsChangeMean  \\\n",
              "0                    NaN   2.65  0.83  4.10  2.862                   0.009468   \n",
              "1                    NaN   3.40  0.53  0.60  2.812                   0.139116   \n",
              "2                    NaN   4.15  0.96  0.78  2.754                  -0.041434   \n",
              "3                    NaN   4.15  0.87  0.66  2.813                   0.034041   \n",
              "4                    NaN   5.15  1.16 -0.64  2.751                   0.082894   \n",
              "\n",
              "   SectorCloseChangeMean  SectorDividendYieldChangeMean  \n",
              "0              -0.017332                       0.028735  \n",
              "1              -0.032962                       0.180859  \n",
              "2               0.015908                      -0.053838  \n",
              "3              -0.035321                       0.072520  \n",
              "4              -0.019228                       0.104132  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-edbf058a-23c3-4a11-b1ab-267cab793e37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>Close</th>\n",
              "      <th>DividendYield</th>\n",
              "      <th>DividendsChange</th>\n",
              "      <th>CloseChange</th>\n",
              "      <th>DividendYieldChange</th>\n",
              "      <th>DividendsChange6M</th>\n",
              "      <th>CloseChange6M</th>\n",
              "      <th>DividendYieldChange6M</th>\n",
              "      <th>Selic</th>\n",
              "      <th>IPCA</th>\n",
              "      <th>IGPM</th>\n",
              "      <th>IFIX</th>\n",
              "      <th>SectorDividendsChangeMean</th>\n",
              "      <th>SectorCloseChangeMean</th>\n",
              "      <th>SectorDividendYieldChangeMean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AFHI11</td>\n",
              "      <td>2021-05</td>\n",
              "      <td>0.60</td>\n",
              "      <td>91.30</td>\n",
              "      <td>0.657174</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>-0.085995</td>\n",
              "      <td>0.312901</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.65</td>\n",
              "      <td>0.83</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.862</td>\n",
              "      <td>0.009468</td>\n",
              "      <td>-0.017332</td>\n",
              "      <td>0.028735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AFHI11</td>\n",
              "      <td>2021-06</td>\n",
              "      <td>0.90</td>\n",
              "      <td>90.30</td>\n",
              "      <td>0.996678</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>-0.010953</td>\n",
              "      <td>0.516612</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.40</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.60</td>\n",
              "      <td>2.812</td>\n",
              "      <td>0.139116</td>\n",
              "      <td>-0.032962</td>\n",
              "      <td>0.180859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AFHI11</td>\n",
              "      <td>2021-07</td>\n",
              "      <td>0.90</td>\n",
              "      <td>93.58</td>\n",
              "      <td>0.961744</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036323</td>\n",
              "      <td>-0.035050</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.15</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.78</td>\n",
              "      <td>2.754</td>\n",
              "      <td>-0.041434</td>\n",
              "      <td>0.015908</td>\n",
              "      <td>-0.053838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AFHI11</td>\n",
              "      <td>2021-08</td>\n",
              "      <td>1.00</td>\n",
              "      <td>93.30</td>\n",
              "      <td>1.071811</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>-0.002992</td>\n",
              "      <td>0.114445</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.15</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.66</td>\n",
              "      <td>2.813</td>\n",
              "      <td>0.034041</td>\n",
              "      <td>-0.035321</td>\n",
              "      <td>0.072520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AFHI11</td>\n",
              "      <td>2021-09</td>\n",
              "      <td>1.05</td>\n",
              "      <td>92.29</td>\n",
              "      <td>1.137718</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>-0.010825</td>\n",
              "      <td>0.061491</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.15</td>\n",
              "      <td>1.16</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>2.751</td>\n",
              "      <td>0.082894</td>\n",
              "      <td>-0.019228</td>\n",
              "      <td>0.104132</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edbf058a-23c3-4a11-b1ab-267cab793e37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-edbf058a-23c3-4a11-b1ab-267cab793e37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-edbf058a-23c3-4a11-b1ab-267cab793e37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_adress.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "Fe6THoP8gRON",
        "outputId": "ea1307cf-08b4-4208-e63b-01764fa531b2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ticker                  Tipo                      Nome  \\\n",
              "0  ABCP11  Shopping Tradicional      Grand Plaza Shopping   \n",
              "1  AIEC11    Prédio Corporativo                 Rochavera   \n",
              "2  AIEC11                Outros         Standard Building   \n",
              "3  ALMI11      Laje Corporativa  Edificio Torre Almirante   \n",
              "4  ALZR11                Outros  Air Liquide São Paulo/SP   \n",
              "\n",
              "            DataCompra  ValorAreaBrutaLocavel Estado          Cidade  \\\n",
              "0  30-05-2009T21:00:00                69503.0     SP     Santo André   \n",
              "1                  NaN                14648.0     SP       São Paulo   \n",
              "2                  NaN                 8341.0     RJ  Rio de Janeiro   \n",
              "3                  NaN                16587.0     RJ  Rio de Janeiro   \n",
              "4                  NaN                 5008.0     SP       São Paulo   \n",
              "\n",
              "                            Endereco  \\\n",
              "0           Avenida Industrial , 600   \n",
              "1  Avenida das Nações Unidas, 14.171   \n",
              "2     Avenida Presidente Wilson, 118   \n",
              "3           Av. Almirante Barroso 81   \n",
              "4        Av. Presidente Wilson 5.874   \n",
              "\n",
              "                                      GoogleMapsLink  PercentualPartic  \\\n",
              "0  https://www.google.com/maps/@-23.6498911,-46.5...              98.6   \n",
              "1  https://www.google.com/maps/place/Regus+-+Sao+...               0.0   \n",
              "2  https://www.google.com/maps/place/Av.+Pres.+Wi...               0.0   \n",
              "3  https://www.google.com/maps/place/Av.+Alm.+Bar...               0.0   \n",
              "4  https://www.google.com/maps/uv?hl=pt-BR&pb=!1s...               0.0   \n",
              "\n",
              "   PecentualVacancia  PercentualInadimplencia90Dias  PercentualFii   Latitude  \\\n",
              "0               3.95                           1.53          97.41 -23.649891   \n",
              "1               0.00                           0.00          69.42 -23.622412   \n",
              "2               0.00                           0.00          30.58 -22.912232   \n",
              "3              58.90                           0.00          89.90 -22.907451   \n",
              "4               0.00                           0.00           4.43 -23.601014   \n",
              "\n",
              "   Longitude  \n",
              "0 -46.532736  \n",
              "1 -46.701606  \n",
              "2 -43.173986  \n",
              "3 -43.175119  \n",
              "4 -46.584170  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76811de0-6caf-4311-9508-e8595868b723\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Tipo</th>\n",
              "      <th>Nome</th>\n",
              "      <th>DataCompra</th>\n",
              "      <th>ValorAreaBrutaLocavel</th>\n",
              "      <th>Estado</th>\n",
              "      <th>Cidade</th>\n",
              "      <th>Endereco</th>\n",
              "      <th>GoogleMapsLink</th>\n",
              "      <th>PercentualPartic</th>\n",
              "      <th>PecentualVacancia</th>\n",
              "      <th>PercentualInadimplencia90Dias</th>\n",
              "      <th>PercentualFii</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABCP11</td>\n",
              "      <td>Shopping Tradicional</td>\n",
              "      <td>Grand Plaza Shopping</td>\n",
              "      <td>30-05-2009T21:00:00</td>\n",
              "      <td>69503.0</td>\n",
              "      <td>SP</td>\n",
              "      <td>Santo André</td>\n",
              "      <td>Avenida Industrial , 600</td>\n",
              "      <td>https://www.google.com/maps/@-23.6498911,-46.5...</td>\n",
              "      <td>98.6</td>\n",
              "      <td>3.95</td>\n",
              "      <td>1.53</td>\n",
              "      <td>97.41</td>\n",
              "      <td>-23.649891</td>\n",
              "      <td>-46.532736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AIEC11</td>\n",
              "      <td>Prédio Corporativo</td>\n",
              "      <td>Rochavera</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14648.0</td>\n",
              "      <td>SP</td>\n",
              "      <td>São Paulo</td>\n",
              "      <td>Avenida das Nações Unidas, 14.171</td>\n",
              "      <td>https://www.google.com/maps/place/Regus+-+Sao+...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>69.42</td>\n",
              "      <td>-23.622412</td>\n",
              "      <td>-46.701606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AIEC11</td>\n",
              "      <td>Outros</td>\n",
              "      <td>Standard Building</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8341.0</td>\n",
              "      <td>RJ</td>\n",
              "      <td>Rio de Janeiro</td>\n",
              "      <td>Avenida Presidente Wilson, 118</td>\n",
              "      <td>https://www.google.com/maps/place/Av.+Pres.+Wi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>30.58</td>\n",
              "      <td>-22.912232</td>\n",
              "      <td>-43.173986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ALMI11</td>\n",
              "      <td>Laje Corporativa</td>\n",
              "      <td>Edificio Torre Almirante</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16587.0</td>\n",
              "      <td>RJ</td>\n",
              "      <td>Rio de Janeiro</td>\n",
              "      <td>Av. Almirante Barroso 81</td>\n",
              "      <td>https://www.google.com/maps/place/Av.+Alm.+Bar...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58.90</td>\n",
              "      <td>0.00</td>\n",
              "      <td>89.90</td>\n",
              "      <td>-22.907451</td>\n",
              "      <td>-43.175119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ALZR11</td>\n",
              "      <td>Outros</td>\n",
              "      <td>Air Liquide São Paulo/SP</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5008.0</td>\n",
              "      <td>SP</td>\n",
              "      <td>São Paulo</td>\n",
              "      <td>Av. Presidente Wilson 5.874</td>\n",
              "      <td>https://www.google.com/maps/uv?hl=pt-BR&amp;pb=!1s...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.43</td>\n",
              "      <td>-23.601014</td>\n",
              "      <td>-46.584170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76811de0-6caf-4311-9508-e8595868b723')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76811de0-6caf-4311-9508-e8595868b723 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76811de0-6caf-4311-9508-e8595868b723');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xzFenxM4xVCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e20133-1230-4908-8d01-8b413fbbde8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162 FIIs restaram na análise:\n",
            "- 60 FIIs de Papel.\n",
            "- 25 FIIs de Hibrido.\n",
            "- 77 FIIs de Tijolo.\n",
            "\n",
            "1091 endereços de FIIs foram encontrados.\n",
            "21.72% dos endereços estão sem Latitude e Longitude.\n"
          ]
        }
      ],
      "source": [
        "# Exibe informações sobre os dados obtidos\n",
        "\n",
        "print(f\"{len(df_history.Ticker.unique())} FIIs restaram na análise:\")\n",
        "\n",
        "for tipo in [\"Papel\", \"Hibrido\", \"Tijolo\"]:\n",
        "    print(f\"- {len(filtra_tipo(df_history, df_funds, tipo).Ticker.unique())} FIIs de {tipo}.\")\n",
        "\n",
        "print(f\"\\n{len(df_adress)} endereços de FIIs foram encontrados.\")\n",
        "\n",
        "percent = df_adress['Latitude'].isnull().sum()/(len(df_adress))*100\n",
        "print(\"%.2f%% dos endereços estão sem Latitude e Longitude.\" % percent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHjJfYeVwdDX"
      },
      "source": [
        "# 2. Data Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wiUpK1uAufh3"
      },
      "outputs": [],
      "source": [
        "def calculate_mae_rmse(abs_errors):\n",
        "\n",
        "    # Garante que teremos apenas 6 casas decimais\n",
        "    abs_errors = [round(x,6) for x in abs_errors]\n",
        "\n",
        "    # Calcular o Mean Absolute Error (MAE)\n",
        "    mae = mean_absolute_error(y_true=np.zeros_like(abs_errors), y_pred=np.array(abs_errors))\n",
        "\n",
        "    # Calcular o Root Mean Squared Error (RMSE)\n",
        "    rmse = mean_squared_error(y_true=np.zeros_like(abs_errors), y_pred=np.array(abs_errors), squared=False)\n",
        "\n",
        "    return round(mae, 6), round(rmse, 6)\n",
        "\n",
        "def upload_errors(pred_col, train_cols, strategy, sectors, abs_errors, params = None):\n",
        "\n",
        "    mae, rmse = calculate_mae_rmse(abs_errors)\n",
        "\n",
        "    with mlflow.start_run(run_name=train_cols):   \n",
        "        \n",
        "        df_abs = pd.DataFrame(abs_errors, columns = ['errors'])\n",
        "\n",
        "        # Parameters\n",
        "        mlflow.log_param(\"pred_col\", unidecode(pred_col))\n",
        "        mlflow.log_param(\"train_cols\", unidecode(train_cols))\n",
        "        mlflow.log_param(\"strategy\", unidecode(strategy))\n",
        "        mlflow.log_param(\"sector\", unidecode(sectors))\n",
        "\n",
        "        # Error Metrics\n",
        "        mlflow.log_metric(\"MAE\", round(mae, 6))\n",
        "        mlflow.log_metric(\"RMSE\", round(rmse, 6))\n",
        "        mlflow.log_metric(\"mean\", df_abs.describe().values[1][0])\n",
        "        mlflow.log_metric(\"std\", df_abs.describe().values[2][0])\n",
        "        mlflow.log_metric(\"min\", df_abs.describe().values[3][0])\n",
        "        mlflow.log_metric(\"25 pct.\", df_abs.describe().values[4][0])\n",
        "        mlflow.log_metric(\"50 pct.\", df_abs.describe().values[5][0])\n",
        "        mlflow.log_metric(\"75 pct.\", df_abs.describe().values[6][0])\n",
        "        mlflow.log_metric(\"max\", df_abs.describe().values[7][0])\n",
        "\n",
        "        # Machine Learning Params\n",
        "        mlflow.log_metric(\"n_estimators\", 0.0 if params == None else params[\"n_estimators\"]) \n",
        "        mlflow.log_metric(\"learning_rate\", 0.0 if params == None else params[\"learning_rate\"]) \n",
        "        mlflow.log_metric(\"max_depth\", 0.0 if params == None else params[\"max_depth\"]) \n",
        "        mlflow.log_metric(\"min_child_weight\", 0.0 if params == None else params[\"min_child_weight\"]) \n",
        "        mlflow.log_metric(\"colsample_bytree\", 0.0 if params == None else params[\"colsample_bytree\"])\n",
        "\n",
        "def get_experiments_result(experiment_name, sort_column = \"\"):\n",
        "\n",
        "    df = mlflow.search_runs([mlflow_experiment._experiment_id])\n",
        "\n",
        "    if(len(df) > 0):\n",
        "        if sort_column in df.columns:\n",
        "            return df.sort_values(by=sort_column, ascending=True)\n",
        "        else:\n",
        "            return df\n",
        "\n",
        "    return df\n",
        "\n",
        "def is_mlfow_configured():\n",
        "    return not (os.environ['MLFLOW_TRACKING_USERNAME'] == \"\" or os.environ['MLFLOW_TRACKING_PASSWORD'] == \"\" or os.environ['MLFLOW_TRACKING_PROJECTNAME'] == \"\")\n",
        "\n",
        "def get_possibilities(target_column, training_columns):\n",
        "\n",
        "    possibilities = []\n",
        "    for L in range(len(training_columns) + 1):\n",
        "        for subset in itertools.combinations(training_columns, L):\n",
        "            possibilities.append([target_column] + list(subset))\n",
        "\n",
        "    possibilities.reverse()\n",
        "\n",
        "    return list(filter(lambda x: len(x) > 4, possibilities)) \n",
        "\n",
        "def train_test_split(data, perc):\n",
        "\n",
        "    data = data.values\n",
        "    n = int(len(data) * (1 - perc))\n",
        "    return data[:n], data[n:]\n",
        "\n",
        "def arima_predict(df_history, fundo, pred_col, pred_index = 1):\n",
        "\n",
        "    df = df_history[df_history['Ticker'] == fundo].copy()\n",
        "    df['Target'] = df[pred_col].shift(-pred_index)\n",
        "    df = df[[pred_col, 'Target']]\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    y_pred = []\n",
        "    train, test = train_test_split(df, 0.4) # 60% de treino\n",
        "\n",
        "    # cria a variável history\n",
        "    history = [x[0] for x in train]\n",
        "\n",
        "    for i in range(len(test)):\n",
        "        test_X, test_y = test[i, :-1], test[i, -1]\n",
        "\n",
        "        # O arima deverá considerar apenas o valor da coluna principal\n",
        "        model = SARIMAX(history, order=(1,1,1), maxiter=1000)\n",
        "        resultado_sarimax = model.fit()\n",
        "\n",
        "        # Obtém a predição de {pred_index} meses à frente\n",
        "        output = resultado_sarimax.get_forecast(steps=pred_index)\n",
        "\n",
        "        # Obtém a predição do mês de interesse\n",
        "        pred = output.predicted_mean[pred_index-1]\n",
        "\n",
        "        y_pred.append(pred)\n",
        "        history.append(test[i][0])\n",
        "\n",
        "    y_real = test[:, -1]\n",
        "\n",
        "    yhat = round(y_pred[0],2)\n",
        "    abs_error = list(np.abs(y_real - y_pred))\n",
        "\n",
        "    return yhat, abs_error\n",
        "\n",
        "def model_predict(train, test_X, val_X, val_y, model):\n",
        "\n",
        "    train = np.array(train)\n",
        "    val_y = np.array([val_y])\n",
        "    val_X = np.array([val_X])\n",
        "    test_X = np.array([test_X])\n",
        "\n",
        "    X, y = train[:, :-1], train[:, -1]\n",
        "\n",
        "    if(len(val_X) > 0):\n",
        "        model.fit(X, y, eval_set=[(X,y),(val_X,val_y)], verbose=0)\n",
        "    else:\n",
        "        model.fit(X, y)\n",
        "    \n",
        "    pred = model.predict(test_X)\n",
        "\n",
        "    return pred[0]\n",
        "\n",
        "def machinelearn_predict(df_history, fundo, pred_col, train_cols, params, pred_index = 1):\n",
        " \n",
        "    df = df_history[df_history['Ticker'] == fundo][train_cols].copy()\n",
        "    df[\"Target\"] = df[pred_col].shift(-pred_index)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    model = None\n",
        "    y_pred = []\n",
        "    train, test = train_test_split(df, 0.4) # 60% de treino\n",
        "\n",
        "    history = [x for x in train]\n",
        "\n",
        "    for i in range(len(test) - 1):\n",
        "\n",
        "        val_X, val_y = test[i, :-1], test[i, -1]\n",
        "        test_X, test_y = test[i + 1, :-1], test[i + 1, -1]\n",
        "\n",
        "        model = XGBRegressor()\n",
        "        model.set_params(**params)\n",
        "\n",
        "        pred = model_predict(history, test_X, val_X, val_y, model)\n",
        "\n",
        "        y_pred.append(pred)\n",
        "\n",
        "        history.append(test[i])\n",
        "    \n",
        "    y_real = test[1:, -1]\n",
        "\n",
        "    yhat = round(y_pred[0],2)\n",
        "    abs_error = list(np.abs(y_real - y_pred))\n",
        "\n",
        "    return yhat, abs_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "existent_experiments = []\n",
        "\n",
        "# Trecho para obter os experimentos que já estão no mlflow\n",
        "if is_mlfow_configured():\n",
        "\n",
        "    mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "    mlflow_experiment = mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    try:\n",
        "        existent_experiments = list(get_experiments_result(experiment_name)[\"params.train_cols\"])\n",
        "    except:\n",
        "        print(\"Nenhum experimento foi encontrado\")"
      ],
      "metadata": {
        "id": "b1El1mOYiiBA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oruj5t7-QM1B",
        "outputId": "77b8ad8c-5491-4d12-e380-275c96f93e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "848 possibilidades serão avaliadas\n"
          ]
        }
      ],
      "source": [
        "# Trecho para obter as combinações de colunas de acordo com a janela de treino\n",
        "pred_column = 'CloseChange'\n",
        "possibilities = get_possibilities(pred_column, (['Close','DividendYield','DividendsChange','DividendYieldChange','SectorDividendsChangeMean','SectorCloseChangeMean','Selic','IPCA','IGPM','IFIX']))\n",
        "\n",
        "if janela_treino == 6:\n",
        "    pred_column = 'CloseChange6M'\n",
        "    possibilities = get_possibilities(pred_column, (['Close','DividendYield','DividendsChange6M','DividendYieldChange6M','SectorDividendsChangeMean','SectorCloseChangeMean','Selic','IPCA','IGPM','IFIX']))\n",
        "\n",
        "print(str(len(possibilities)) + \" possibilidades serão avaliadas\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtem os Tickers de cada tipo\n",
        "papel = list(filtra_tipo(df_history, df_funds, 'Papel').Ticker.unique())\n",
        "tijolo = list(filtra_tipo(df_history, df_funds, 'Tijolo').Ticker.unique())\n",
        "hibrido = list(filtra_tipo(df_history, df_funds, 'Hibrido').Ticker.unique())"
      ],
      "metadata": {
        "id": "1qG8aJyZjTkp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtra o dataframe de acordo com o tipo que se deseja treinar\n",
        "df_history = filtra_tipo(df_history, df_funds, tipo_interesse)\n",
        "sectors = pd.merge(df_funds, df_history, on='Ticker')[\"Setor\"].unique()"
      ],
      "metadata": {
        "id": "7psl0yWvFXVZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WHaiq-e3spev",
        "outputId": "37a0a4e0-266f-4b31-ec71-407254accf9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating \"CloseChange, Close, DividendYield, DividendsChange, DividendYieldChange, SectorDividendsChangeMean, SectorCloseChangeMean, Selic, IPCA, IGPM, IFIX\": 1/848\n",
            "Calculating errors of ARCT11: 1/25\n",
            "Calculating errors of BREV11: 2/25\n",
            "Calculating errors of CPFF11: 3/25\n",
            "Calculating errors of CXRI11: 4/25\n",
            "Calculating errors of FATN11: 5/25\n",
            "Calculating errors of FLMA11: 6/25\n",
            "Calculating errors of GALG11: 7/25\n",
            "Calculating errors of HBRH11: 8/25\n",
            "Calculating errors of HGRU11: 9/25\n",
            "Calculating errors of HOSI11: 10/25\n",
            "Calculating errors of JSRE11: 11/25\n",
            "Calculating errors of KNRI11: 12/25\n",
            "Calculating errors of MFAI11: 13/25\n",
            "Calculating errors of MFII11: 14/25\n",
            "Calculating errors of MXRF11: 15/25\n",
            "Calculating errors of NEWL11: 16/25\n",
            "Calculating errors of PATL11: 17/25\n",
            "Calculating errors of PLCR11: 18/25\n",
            "Calculating errors of QAMI11: 19/25\n",
            "Calculating errors of RBRD11: 20/25\n",
            "Calculating errors of RBRS11: 21/25\n",
            "Calculating errors of RECT11: 22/25\n",
            "Calculating errors of RELG11: 23/25\n",
            "Calculating errors of RZTR11: 24/25\n",
            "Calculating errors of SARE11: 25/25\n",
            "(MAE, RMSE): (0.05563, 0.073202)\n",
            "\n",
            "Calculating \"CloseChange, DividendYield, DividendsChange, DividendYieldChange, SectorDividendsChangeMean, SectorCloseChangeMean, Selic, IPCA, IGPM, IFIX\": 2/848\n",
            "Calculating errors of ARCT11: 1/25\n",
            "Calculating errors of BREV11: 2/25\n",
            "Calculating errors of CPFF11: 3/25\n",
            "Calculating errors of CXRI11: 4/25\n",
            "Calculating errors of FATN11: 5/25\n",
            "Calculating errors of FLMA11: 6/25\n",
            "Calculating errors of GALG11: 7/25\n",
            "Calculating errors of HBRH11: 8/25\n",
            "Calculating errors of HGRU11: 9/25\n",
            "Calculating errors of HOSI11: 10/25\n",
            "Calculating errors of JSRE11: 11/25\n",
            "Calculating errors of KNRI11: 12/25\n",
            "Calculating errors of MFAI11: 13/25\n",
            "Calculating errors of MFII11: 14/25\n",
            "Calculating errors of MXRF11: 15/25\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4a3359e12a51>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculating errors of \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mfundo\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Ticker\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmachinelearn_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfundo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_possibility\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjanela_treino\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mabs_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs_errors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mabs_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-fdf4bd7727b1>\u001b[0m in \u001b[0;36mmachinelearn_predict\u001b[0;34m(df_history, fundo, pred_col, train_cols, params, pred_index)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-fdf4bd7727b1>\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(train, test_X, val_X, val_y, model)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         )\n\u001b[0;32m--> 961\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1779\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                                                     dtrain.handle))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "params = {\"n_estimators\" : 4000, \"early_stopping_rounds\" : 100, \"learning_rate\": 0.1, \"max_depth\": 6,\"min_child_weight\" : 1,\"colsample_bytree\" : 0.4}\n",
        "\n",
        "# Realiza o treino de todas as combinações com XGboost\n",
        "for j, array_possibility in enumerate(possibilities):\n",
        "    \n",
        "    abs_errors = []\n",
        "    string_possibility =  \", \".join(array_possibility)\n",
        "\n",
        "    print('Calculating \"'+ string_possibility + '\": ' + str(j+1) + '/' + str(len(possibilities)))\n",
        "\n",
        "    if string_possibility not in existent_experiments:\n",
        "\n",
        "        for i, fundo in enumerate(df_history[\"Ticker\"].unique()):\n",
        "\n",
        "            print(\"Calculating errors of \"+ fundo + \": \" + str(i+1) + \"/\" + str(len(df_history[\"Ticker\"].unique())))\n",
        "\n",
        "            prediction, abs_err = machinelearn_predict(df_history, fundo, pred_column, array_possibility, params, janela_treino)\n",
        "            abs_errors = abs_errors + abs_err\n",
        "\n",
        "        print(f'(MAE, RMSE): {calculate_mae_rmse(abs_errors)}\\n')\n",
        "\n",
        "        if is_mlfow_configured():\n",
        "            upload_errors(pred_column, string_possibility, 'Xgboost', ', '.join(sectors), abs_errors, params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abs_errors = []\n",
        "\n",
        "# Realiza o treino do ARIMA\n",
        "if pred_column not in existent_experiments:\n",
        "\n",
        "    print('Calculating ARIMA...')\n",
        "\n",
        "    for i, fundo in enumerate(df_history[\"Ticker\"].unique()):\n",
        "\n",
        "        print(\"Calculating errors of \"+ fundo + \": \" + str(i+1) + \"/\" + str(len(df_history[\"Ticker\"].unique())))\n",
        "\n",
        "        prediction, abs_err = arima_predict(df_history, fundo, pred_column, janela_treino)\n",
        "        abs_errors = abs_errors + abs_err\n",
        "\n",
        "    print(f'(MAE, RMSE): {calculate_mae_rmse(abs_errors)}\\n')\n",
        "    \n",
        "    if is_mlfow_configured():\n",
        "        upload_errors(pred_column, pred_column, 'Arima', ', '.join(sectors), abs_errors)"
      ],
      "metadata": {
        "id": "ZeK40jWB-yxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8407a6e9-5c35-49c7-83a5-06ca189900f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating ARIMA...\n",
            "Calculating errors of ARCT11: 1/25\n",
            "Calculating errors of BREV11: 2/25\n",
            "Calculating errors of CPFF11: 3/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of CXRI11: 4/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of FATN11: 5/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of FLMA11: 6/25\n",
            "Calculating errors of GALG11: 7/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of HBRH11: 8/25\n",
            "Calculating errors of HGRU11: 9/25\n",
            "Calculating errors of HOSI11: 10/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of JSRE11: 11/25\n",
            "Calculating errors of KNRI11: 12/25\n",
            "Calculating errors of MFAI11: 13/25\n",
            "Calculating errors of MFII11: 14/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of MXRF11: 15/25\n",
            "Calculating errors of NEWL11: 16/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of PATL11: 17/25\n",
            "Calculating errors of PLCR11: 18/25\n",
            "Calculating errors of QAMI11: 19/25\n",
            "Calculating errors of RBRD11: 20/25\n",
            "Calculating errors of RBRS11: 21/25\n",
            "Calculating errors of RECT11: 22/25\n",
            "Calculating errors of RELG11: 23/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating errors of RZTR11: 24/25\n",
            "Calculating errors of SARE11: 25/25\n",
            "(MAE, RMSE): (0.039089, 0.055523)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}